#!/usr/bin/env python3
"""
LLMå¢å¼ºå¯è§£é‡Šæ€§æ¨¡å—
ç»“åˆä¼ ç»Ÿå¯è§£é‡Šæ€§æ–¹æ³•å’Œå¤§è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›
æä¾›æ›´å…¨é¢ã€æ›´æ˜“ç†è§£çš„å†³ç­–è§£é‡Š
"""

import json
import torch
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import requests
import time
from abc import ABC, abstractmethod

@dataclass
class ExplanationContext:
    """è§£é‡Šä¸Šä¸‹æ–‡æ•°æ®ç»“æ„"""
    video_id: str
    model_prediction: torch.Tensor
    confidence_score: float
    
    # ä¼ ç»Ÿå¯è§£é‡Šæ€§ä¿¡æ¯
    modality_weights: Dict[str, float]
    feature_importance: Dict[str, np.ndarray]
    fake_regions: Optional[np.ndarray]
    attention_maps: Dict[str, np.ndarray]
    
    # ç¥ç»ç¬¦å·è§„åˆ™ä¿¡æ¯
    neural_symbolic_info: Dict[str, Any]
    matched_rules: List[Dict[str, Any]]
    rule_application_history: List[Dict[str, Any]]
    
    # éšå¼æ„è§åˆ†æ
    implicit_opinion_analysis: Dict[str, Any]
    feature_analysis: Dict[str, Any]
    
    # å…ƒæ•°æ®
    video_metadata: Optional[Dict[str, Any]] = None
    processing_time: Optional[float] = None

class LLMProvider(ABC):
    """LLMæä¾›è€…æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def generate_explanation(self, prompt: str, context: ExplanationContext) -> str:
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        pass

class OpenAIProvider(LLMProvider):
    """OpenAI GPTæä¾›è€…"""
    
    def __init__(self, api_key: str, model: str = "gpt-4"):
        self.api_key = api_key
        self.model = model
        self.base_url = "https://api.openai.com/v1/chat/completions"
    
    def generate_explanation(self, prompt: str, context: ExplanationContext) -> str:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è™šå‡è§†é¢‘æ£€æµ‹è§£é‡Šä¸“å®¶ï¼Œèƒ½å¤ŸåŸºäºæŠ€æœ¯åˆ†æç»“æœæä¾›æ¸…æ™°ã€å‡†ç¡®å’Œæœ‰æ´å¯ŸåŠ›çš„è§£é‡Šã€‚"},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 1000,
            "temperature": 0.3
        }
        
        try:
            response = requests.post(self.base_url, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content']
        except Exception as e:
            return f"LLMè§£é‡Šç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def is_available(self) -> bool:
        return bool(self.api_key)

class LocalLLMProvider(LLMProvider):
    """æœ¬åœ°LLMæä¾›è€…ï¼ˆæ”¯æŒOllamaç­‰ï¼‰"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "llama2"):
        self.base_url = base_url
        self.model = model
    
    def generate_explanation(self, prompt: str, context: ExplanationContext) -> str:
        try:
            data = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": 0.3, "num_predict": 800}
            }
            
            response = requests.post(f"{self.base_url}/api/generate", json=data, timeout=60)
            response.raise_for_status()
            return response.json().get('response', 'æœ¬åœ°LLMå“åº”è§£æå¤±è´¥')
        except Exception as e:
            return f"æœ¬åœ°LLMè§£é‡Šç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def is_available(self) -> bool:
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False

class HybridExplainer:
    """æ··åˆå¯è§£é‡Šæ€§ç³»ç»Ÿ - ç»“åˆä¼ ç»Ÿæ–¹æ³•å’ŒLLM"""
    
    def __init__(self, 
                 llm_provider: LLMProvider,
                 fallback_to_traditional: bool = True,
                 cache_explanations: bool = True,
                 explanation_templates: Optional[Dict[str, str]] = None):
        """
        åˆå§‹åŒ–æ··åˆè§£é‡Šå™¨
        
        Args:
            llm_provider: LLMæä¾›è€…
            fallback_to_traditional: å½“LLMä¸å¯ç”¨æ—¶æ˜¯å¦å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            cache_explanations: æ˜¯å¦ç¼“å­˜è§£é‡Šç»“æœ
            explanation_templates: è‡ªå®šä¹‰è§£é‡Šæ¨¡æ¿
        """
        self.llm_provider = llm_provider
        self.fallback_to_traditional = fallback_to_traditional
        self.cache_explanations = cache_explanations
        self.explanation_cache = {}
        
        # é»˜è®¤è§£é‡Šæ¨¡æ¿
        self.templates = explanation_templates or {
            "decision_summary": self._get_decision_summary_template(),
            "rule_reasoning": self._get_rule_reasoning_template(),
            "confidence_analysis": self._get_confidence_analysis_template(),
            "risk_assessment": self._get_risk_assessment_template()
        }
    
    def generate_comprehensive_explanation(self, context: ExplanationContext) -> Dict[str, Any]:
        """
        ç”Ÿæˆç»¼åˆè§£é‡ŠæŠ¥å‘Š
        
        Args:
            context: è§£é‡Šä¸Šä¸‹æ–‡
            
        Returns:
            åŒ…å«å¤šå±‚æ¬¡è§£é‡Šçš„å­—å…¸
        """
        start_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(context)
        if self.cache_explanations and cache_key in self.explanation_cache:
            return self.explanation_cache[cache_key]
        
        explanation_result = {
            "video_id": context.video_id,
            "timestamp": time.time(),
            "model_prediction": {
                "class": torch.argmax(context.model_prediction).item(),
                "confidence": context.confidence_score,
                "probabilities": context.model_prediction.tolist()
            }
        }
        
        # 1. ç”Ÿæˆå†³ç­–æ‘˜è¦
        explanation_result["decision_summary"] = self._generate_decision_summary(context)
        
        # 2. ç¥ç»ç¬¦å·è§„åˆ™æ¨ç†
        explanation_result["rule_reasoning"] = self._generate_rule_reasoning(context)
        
        # 3. ç½®ä¿¡åº¦åˆ†æ
        explanation_result["confidence_analysis"] = self._generate_confidence_analysis(context)
        
        # 4. é£é™©è¯„ä¼°
        explanation_result["risk_assessment"] = self._generate_risk_assessment(context)
        
        # 5. ä¼ ç»Ÿå¯è§£é‡Šæ€§è¡¥å……
        explanation_result["technical_details"] = self._generate_technical_details(context)
        
        # 6. ç”¨æˆ·å‹å¥½æ€»ç»“
        explanation_result["user_friendly_summary"] = self._generate_user_summary(context, explanation_result)
        
        explanation_result["processing_time"] = time.time() - start_time
        
        # ç¼“å­˜ç»“æœ
        if self.cache_explanations:
            self.explanation_cache[cache_key] = explanation_result
        
        return explanation_result
    
    def _generate_decision_summary(self, context: ExplanationContext) -> Dict[str, Any]:
        """ç”Ÿæˆå†³ç­–æ‘˜è¦"""
        prompt = self.templates["decision_summary"].format(
            video_id=context.video_id,
            prediction=torch.argmax(context.model_prediction).item(),
            confidence=context.confidence_score,
            modality_weights=json.dumps(context.modality_weights, ensure_ascii=False, indent=2),
            neural_symbolic_info=json.dumps(context.neural_symbolic_info, ensure_ascii=False, indent=2)
        )
        
        if self.llm_provider.is_available():
            llm_explanation = self.llm_provider.generate_explanation(prompt, context)
            return {
                "source": "llm",
                "explanation": llm_explanation,
                "raw_data": {
                    "prediction_class": torch.argmax(context.model_prediction).item(),
                    "confidence": context.confidence_score
                }
            }
        elif self.fallback_to_traditional:
            return self._traditional_decision_summary(context)
        else:
            return {"source": "unavailable", "explanation": "LLMæœåŠ¡ä¸å¯ç”¨"}
    
    def _generate_rule_reasoning(self, context: ExplanationContext) -> Dict[str, Any]:
        """ç”Ÿæˆè§„åˆ™æ¨ç†è§£é‡Š"""
        if not context.neural_symbolic_info or not context.matched_rules:
            return {"source": "none", "explanation": "æœªåº”ç”¨ç¥ç»ç¬¦å·è§„åˆ™"}
        
        prompt = self.templates["rule_reasoning"].format(
            matched_rules=json.dumps(context.matched_rules, ensure_ascii=False, indent=2),
            rule_application=json.dumps(context.neural_symbolic_info, ensure_ascii=False, indent=2),
            implicit_analysis=json.dumps(context.implicit_opinion_analysis, ensure_ascii=False, indent=2)
        )
        
        if self.llm_provider.is_available():
            llm_explanation = self.llm_provider.generate_explanation(prompt, context)
            return {
                "source": "llm",
                "explanation": llm_explanation,
                "applied_rules": len(context.matched_rules),
                "rule_confidence": context.neural_symbolic_info.get("confidence_boost", 0)
            }
        elif self.fallback_to_traditional:
            return self._traditional_rule_reasoning(context)
        else:
            return {"source": "unavailable", "explanation": "LLMæœåŠ¡ä¸å¯ç”¨"}
    
    def _generate_confidence_analysis(self, context: ExplanationContext) -> Dict[str, Any]:
        """ç”Ÿæˆç½®ä¿¡åº¦åˆ†æ"""
        prompt = self.templates["confidence_analysis"].format(
            confidence=context.confidence_score,
            feature_analysis=json.dumps(context.feature_analysis, ensure_ascii=False, indent=2),
            modality_weights=json.dumps(context.modality_weights, ensure_ascii=False, indent=2)
        )
        
        if self.llm_provider.is_available():
            llm_explanation = self.llm_provider.generate_explanation(prompt, context)
            return {
                "source": "llm", 
                "explanation": llm_explanation,
                "confidence_level": self._categorize_confidence(context.confidence_score)
            }
        elif self.fallback_to_traditional:
            return self._traditional_confidence_analysis(context)
        else:
            return {"source": "unavailable", "explanation": "LLMæœåŠ¡ä¸å¯ç”¨"}
    
    def _generate_risk_assessment(self, context: ExplanationContext) -> Dict[str, Any]:
        """ç”Ÿæˆé£é™©è¯„ä¼°"""
        prompt = self.templates["risk_assessment"].format(
            prediction=torch.argmax(context.model_prediction).item(),
            confidence=context.confidence_score,
            implicit_analysis=json.dumps(context.implicit_opinion_analysis, ensure_ascii=False, indent=2)
        )
        
        if self.llm_provider.is_available():
            llm_explanation = self.llm_provider.generate_explanation(prompt, context)
            return {
                "source": "llm",
                "explanation": llm_explanation,
                "risk_level": self._calculate_risk_level(context)
            }
        elif self.fallback_to_traditional:
            return self._traditional_risk_assessment(context)
        else:
            return {"source": "unavailable", "explanation": "LLMæœåŠ¡ä¸å¯ç”¨"}
    
    def _generate_technical_details(self, context: ExplanationContext) -> Dict[str, Any]:
        """ç”ŸæˆæŠ€æœ¯ç»†èŠ‚ï¼ˆä¼ ç»Ÿå¯è§£é‡Šæ€§ï¼‰"""
        return {
            "modality_contributions": context.modality_weights,
            "feature_importance_stats": {
                modality: {
                    "max": float(np.max(importance)),
                    "mean": float(np.mean(importance)),
                    "std": float(np.std(importance))
                }
                for modality, importance in context.feature_importance.items()
            },
            "attention_statistics": {
                key: {
                    "shape": attention.shape,
                    "max_attention": float(np.max(attention)),
                    "attention_entropy": float(-np.sum(attention * np.log(attention + 1e-8)))
                }
                for key, attention in context.attention_maps.items()
            },
            "neural_symbolic_metrics": context.neural_symbolic_info
        }
    
    def _generate_user_summary(self, context: ExplanationContext, full_explanation: Dict[str, Any]) -> str:
        """ç”Ÿæˆç”¨æˆ·å‹å¥½çš„æ€»ç»“"""
        prediction_class = torch.argmax(context.model_prediction).item()
        prediction_text = "è™šå‡è§†é¢‘" if prediction_class == 1 else "çœŸå®è§†é¢‘"
        confidence_text = f"{context.confidence_score*100:.1f}%"
        
        summary_parts = [
            f"ğŸ¯ **æ£€æµ‹ç»“æœ**: {prediction_text} (ç½®ä¿¡åº¦: {confidence_text})",
        ]
        
        # æ·»åŠ ä¸»è¦ä¾æ®
        if context.modality_weights:
            dominant_modality = max(context.modality_weights.items(), key=lambda x: x[1])
            summary_parts.append(f"ğŸ“Š **ä¸»è¦ä¾æ®**: {dominant_modality[0]}æ¨¡æ€ ({dominant_modality[1]*100:.1f}%è´¡çŒ®åº¦)")
        
        # æ·»åŠ è§„åˆ™åº”ç”¨æƒ…å†µ
        if context.neural_symbolic_info and context.neural_symbolic_info.get("matched_rules_count", 0) > 0:
            rule_count = context.neural_symbolic_info["matched_rules_count"]
            summary_parts.append(f"âš–ï¸ **è§„åˆ™åŒ¹é…**: åº”ç”¨äº†{rule_count}æ¡ç¥ç»ç¬¦å·è§„åˆ™")
        
        # æ·»åŠ é£é™©ç­‰çº§
        risk_level = full_explanation.get("risk_assessment", {}).get("risk_level", "æœªçŸ¥")
        summary_parts.append(f"âš ï¸ **é£é™©ç­‰çº§**: {risk_level}")
        
        return "\n".join(summary_parts)
    
    def _get_decision_summary_template(self) -> str:
        return """
åŸºäºä»¥ä¸‹æŠ€æœ¯åˆ†æç»“æœï¼Œè¯·ç”Ÿæˆä¸€ä¸ªæ¸…æ™°çš„å†³ç­–æ‘˜è¦è§£é‡Šï¼š

è§†é¢‘ID: {video_id}
é¢„æµ‹ç»“æœ: {prediction} (0=çœŸå®, 1=è™šå‡)
ç½®ä¿¡åº¦: {confidence:.3f}

æ¨¡æ€æƒé‡åˆ†æ:
{modality_weights}

ç¥ç»ç¬¦å·è§„åˆ™ä¿¡æ¯:
{neural_symbolic_info}

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šï¼š
1. æ¨¡å‹ä¸ºä»€ä¹ˆåšå‡ºè¿™ä¸ªåˆ¤æ–­ï¼Ÿ
2. å“ªäº›å› ç´ æ˜¯å†³å®šæ€§çš„ï¼Ÿ
3. è¿™ä¸ªåˆ¤æ–­çš„å¯é æ€§å¦‚ä½•ï¼Ÿ

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒå®¢è§‚å’Œä¸“ä¸šã€‚
"""
    
    def _get_rule_reasoning_template(self) -> str:
        return """
åŸºäºä»¥ä¸‹ç¥ç»ç¬¦å·è§„åˆ™åº”ç”¨æƒ…å†µï¼Œè¯·è§£é‡Šè§„åˆ™æ¨ç†è¿‡ç¨‹ï¼š

åŒ¹é…çš„è§„åˆ™:
{matched_rules}

è§„åˆ™åº”ç”¨ç»“æœ:
{rule_application}

éšå¼æ„è§åˆ†æ:
{implicit_analysis}

è¯·è§£é‡Šï¼š
1. å“ªäº›è§„åˆ™è¢«è§¦å‘äº†ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
2. è¿™äº›è§„åˆ™å¦‚ä½•å½±å“æœ€ç»ˆåˆ¤æ–­ï¼Ÿ
3. è§„åˆ™åº”ç”¨çš„åˆç†æ€§å¦‚ä½•ï¼Ÿ

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œé‡ç‚¹è§£é‡Šè§„åˆ™é€»è¾‘ã€‚
"""
    
    def _get_confidence_analysis_template(self) -> str:
        return """
åŸºäºä»¥ä¸‹ä¿¡æ¯åˆ†ææ¨¡å‹ç½®ä¿¡åº¦ï¼š

å½“å‰ç½®ä¿¡åº¦: {confidence:.3f}

ç‰¹å¾åˆ†æ:
{feature_analysis}

æ¨¡æ€æƒé‡:
{modality_weights}

è¯·åˆ†æï¼š
1. è¿™ä¸ªç½®ä¿¡åº¦æ°´å¹³æ„å‘³ç€ä»€ä¹ˆï¼Ÿ
2. å“ªäº›å› ç´ å½±å“äº†ç½®ä¿¡åº¦ï¼Ÿ
3. æ˜¯å¦å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Ÿ

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£ç½®ä¿¡åº¦çš„å«ä¹‰ã€‚
"""
    
    def _get_risk_assessment_template(self) -> str:
        return """
åŸºäºæ£€æµ‹ç»“æœè¿›è¡Œé£é™©è¯„ä¼°ï¼š

é¢„æµ‹ç»“æœ: {prediction}
ç½®ä¿¡åº¦: {confidence:.3f}

è¯¦ç»†åˆ†æ:
{implicit_analysis}

è¯·è¯„ä¼°ï¼š
1. å¦‚æœåˆ¤æ–­é”™è¯¯çš„æ½œåœ¨é£é™©æ˜¯ä»€ä¹ˆï¼Ÿ
2. å»ºè®®é‡‡å–ä»€ä¹ˆåç»­è¡ŒåŠ¨ï¼Ÿ
3. éœ€è¦äººå·¥æ ¸æŸ¥å—ï¼Ÿ

è¯·æä¾›å®ç”¨çš„é£é™©è¯„ä¼°å’Œå»ºè®®ã€‚
"""
    
    # ä¼ ç»Ÿæ–¹æ³•å›é€€å‡½æ•°
    def _traditional_decision_summary(self, context: ExplanationContext) -> Dict[str, Any]:
        prediction = torch.argmax(context.model_prediction).item()
        prediction_text = "è™šå‡è§†é¢‘" if prediction == 1 else "çœŸå®è§†é¢‘"
        
        explanation = f"æ¨¡å‹é¢„æµ‹è¿™æ˜¯{prediction_text}ï¼Œç½®ä¿¡åº¦ä¸º{context.confidence_score:.3f}ã€‚"
        
        if context.modality_weights:
            dominant_modality = max(context.modality_weights.items(), key=lambda x: x[1])
            explanation += f" ä¸»è¦åŸºäº{dominant_modality[0]}æ¨¡æ€çš„è¯æ®ï¼ˆè´¡çŒ®åº¦{dominant_modality[1]:.2f}ï¼‰ã€‚"
        
        return {
            "source": "traditional",
            "explanation": explanation,
            "raw_data": {"prediction_class": prediction, "confidence": context.confidence_score}
        }
    
    def _traditional_rule_reasoning(self, context: ExplanationContext) -> Dict[str, Any]:
        rule_count = len(context.matched_rules) if context.matched_rules else 0
        explanation = f"åº”ç”¨äº†{rule_count}æ¡ç¥ç»ç¬¦å·è§„åˆ™ã€‚"
        
        if context.neural_symbolic_info:
            bias_adj = context.neural_symbolic_info.get("bias_adjustment", 0)
            if abs(bias_adj) > 0.01:
                explanation += f" è§„åˆ™è°ƒæ•´äº†é¢„æµ‹åç½®{bias_adj:.3f}ã€‚"
        
        return {
            "source": "traditional",
            "explanation": explanation,
            "applied_rules": rule_count
        }
    
    def _traditional_confidence_analysis(self, context: ExplanationContext) -> Dict[str, Any]:
        confidence_level = self._categorize_confidence(context.confidence_score)
        explanation = f"ç½®ä¿¡åº¦ä¸º{context.confidence_score:.3f}ï¼Œå±äº{confidence_level}æ°´å¹³ã€‚"
        
        return {
            "source": "traditional",
            "explanation": explanation,
            "confidence_level": confidence_level
        }
    
    def _traditional_risk_assessment(self, context: ExplanationContext) -> Dict[str, Any]:
        risk_level = self._calculate_risk_level(context)
        prediction = torch.argmax(context.model_prediction).item()
        
        if prediction == 1 and context.confidence_score > 0.8:
            explanation = "é«˜ç½®ä¿¡åº¦æ£€æµ‹åˆ°è™šå‡è§†é¢‘ï¼Œå»ºè®®è¿›ä¸€æ­¥å®¡æŸ¥ã€‚"
        elif prediction == 1 and context.confidence_score < 0.6:
            explanation = "æ£€æµ‹åˆ°å¯èƒ½çš„è™šå‡è§†é¢‘ï¼Œä½†ç½®ä¿¡åº¦ä¸é«˜ï¼Œå»ºè®®äººå·¥æ ¸æŸ¥ã€‚"
        else:
            explanation = "é¢„æµ‹ä¸ºçœŸå®è§†é¢‘ï¼Œé£é™©è¾ƒä½ã€‚"
        
        return {
            "source": "traditional",
            "explanation": explanation,
            "risk_level": risk_level
        }
    
    # è¾…åŠ©å‡½æ•°
    def _categorize_confidence(self, confidence: float) -> str:
        if confidence >= 0.9:
            return "é«˜"
        elif confidence >= 0.7:
            return "ä¸­ç­‰"
        elif confidence >= 0.5:
            return "è¾ƒä½"
        else:
            return "å¾ˆä½"
    
    def _calculate_risk_level(self, context: ExplanationContext) -> str:
        prediction = torch.argmax(context.model_prediction).item()
        confidence = context.confidence_score
        
        if prediction == 1:  # è™šå‡è§†é¢‘
            if confidence >= 0.8:
                return "é«˜é£é™©"
            elif confidence >= 0.6:
                return "ä¸­ç­‰é£é™©"
            else:
                return "ä½é£é™©"
        else:  # çœŸå®è§†é¢‘
            if confidence >= 0.8:
                return "ä½é£é™©"
            else:
                return "éœ€è¦å…³æ³¨"
    
    def _generate_cache_key(self, context: ExplanationContext) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{context.video_id}_{hash(str(context.model_prediction.tolist()))}"

# å·¥å‚å‡½æ•°
def create_llm_explainer(provider_type: str = "local", **kwargs) -> HybridExplainer:
    """
    åˆ›å»ºLLMè§£é‡Šå™¨çš„å·¥å‚å‡½æ•°
    
    Args:
        provider_type: "openai" æˆ– "local"
        **kwargs: æä¾›è€…ç‰¹å®šçš„å‚æ•°
    """
    if provider_type == "openai":
        api_key = kwargs.get("api_key")
        if not api_key:
            raise ValueError("OpenAI provider requires api_key")
        provider = OpenAIProvider(api_key, kwargs.get("model", "gpt-4"))
    elif provider_type == "local":
        provider = LocalLLMProvider(
            kwargs.get("base_url", "http://localhost:11434"),
            kwargs.get("model", "llama2")
        )
    else:
        raise ValueError(f"Unsupported provider type: {provider_type}")
    
    return HybridExplainer(
        llm_provider=provider,
        fallback_to_traditional=kwargs.get("fallback_to_traditional", True),
        cache_explanations=kwargs.get("cache_explanations", True)
    )

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("LLMå¢å¼ºå¯è§£é‡Šæ€§æ¨¡å—å·²åˆ›å»º")
    print("æ”¯æŒçš„åŠŸèƒ½ï¼š")
    print("- åŸºäºLLMçš„è¯­ä¹‰è§£é‡Š")
    print("- ä¼ ç»Ÿå¯è§£é‡Šæ€§æ–¹æ³•å›é€€")
    print("- å¤šå±‚æ¬¡è§£é‡Šï¼ˆå†³ç­–æ‘˜è¦ã€è§„åˆ™æ¨ç†ã€ç½®ä¿¡åº¦åˆ†æã€é£é™©è¯„ä¼°ï¼‰")
    print("- è§£é‡Šç»“æœç¼“å­˜")
    print("- æ”¯æŒOpenAIå’Œæœ¬åœ°LLM") 