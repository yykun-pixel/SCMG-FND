import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from .Multimodal_Model import Text_Noise_Pre, Audio_Noise_Pre, Visual_Noise_Pre
from .ExplainableDetection import ExplainableDetection
from src.CrossmodalTransformer import MULTModel
from src.StoG import CapsuleSequenceToGraph
from modules.NeuralSymbolicRules import NeuralSymbolicRuleEngine, ImplicitOpinionAnalyzer
from typing import Dict, List, Tuple, Optional, Union
import matplotlib.pyplot as plt
import math
import gc


def extract(v, t, x_shape):
    """
    Extract some coefficients at specified timesteps, then reshape to
    [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.
    """
    device = t.device
    out = torch.gather(v, index=t, dim=0).float().to(device)
    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))


class GaussianDiffusionTrainer(nn.Module):
    def __init__(self, modelConfig, beta_1, beta_T, T, t_in, a_in, v_in, d_m, dropout, label_dim,
                 unified_size, vertex_num, routing, T_t, T_a, T_v,  batch_size):
        super().__init__()

        self.T = T
        self.batch_size = batch_size
        self.mult_dropout = dropout
        self.unified_size = unified_size
        self.vertex_num = vertex_num

        self.register_buffer(
            'betas', torch.linspace(beta_1, beta_T, T).double())
        alphas = 1. - self.betas
        alphas_bar = torch.cumprod(alphas, dim=0)

        # calculations for diffusion q(x_t | x_{t-1}) and others
        self.register_buffer(
            'sqrt_alphas_bar', torch.sqrt(alphas_bar))
        self.register_buffer(
            'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))

        # Feature Extraction
        self.fc_pre_t_1 = nn.LSTM(t_in, modelConfig["t_in_pre"], bidirectional=True)
        self.fc_pre_t_2 = nn.Linear(modelConfig["t_in_pre"]*2, modelConfig["t_in_pre"])
        self.fc_pre_v = torch.nn.Linear(v_in, modelConfig["v_in_pre"])
        self.fc_pre_com = nn.Sequential(torch.nn.Linear(modelConfig["t_in"], unified_size), torch.nn.ReLU(), nn.Dropout(p=modelConfig["comments_dropout"]))
        self.fc_pre_user = nn.Sequential(torch.nn.Linear(modelConfig["t_in"], unified_size), torch.nn.ReLU(),
                                        nn.Dropout(p=modelConfig["comments_dropout"]))
        self.fc_pre_c3d = torch.nn.Linear(modelConfig["c3d_in"], unified_size)
        self.fc_pre_gpt_1 = nn.LSTM(t_in, modelConfig["t_in_pre"], bidirectional=True)
        self.fc_pre_gpt_2 = nn.Linear(modelConfig["t_in_pre"] * 2, modelConfig["t_in_pre"])
        
        # æ·»åŠ ä¸€ä¸ªæŠ•å½±å±‚ï¼Œç”¨äºå°†videos_globalä»v_in_preç»´åº¦æ˜ å°„åˆ°unified_sizeç»´åº¦
        self.videos_global_proj = torch.nn.Linear(modelConfig["v_in_pre"], unified_size)

        self.vggish_layer = torch.hub.load(r'torchvggish-master', 'vggish', source='local')
        net_structure = list(self.vggish_layer.children())
        self.vggish_modified = nn.Sequential(*net_structure[-2:-1])
        self.fc_pre_a = nn.Linear(a_in, modelConfig["a_in_pre"])

        # Intra-modal Enhancement
        self.fc_g_t = nn.Linear(d_m * 6, d_m)
        self.fc_a_MTout = nn.Linear(d_m * 3, d_m)
        self.fc_v_MTout = nn.Linear(d_m * 3, d_m)
        self.CrossmodalTransformer = MULTModel(modelConfig["t_in_pre"], modelConfig["a_in_pre"], modelConfig["v_in_pre"], d_m, self.mult_dropout)
        self.StoG = CapsuleSequenceToGraph(d_m, unified_size, vertex_num, routing, T_t, T_a, T_v)

        # Cross-modal Interaction
        self.model_t = Text_Noise_Pre(T=modelConfig["T"], ch=modelConfig["vertex_num"],
                           dropout=modelConfig["Text_Pre_dropout"],
                           in_ch=unified_size)
        self.model_a = Audio_Noise_Pre(T=modelConfig["T"], ch=modelConfig["vertex_num"],
                           dropout=modelConfig["Img_Pre_dropout"],
                           in_ch=unified_size)
        self.model_v = Visual_Noise_Pre(T=modelConfig["T"], ch=modelConfig["vertex_num"],
                                       dropout=modelConfig["Img_Pre_dropout"],
                                       in_ch=unified_size)

        self.fc_t = nn.Linear(in_features=vertex_num, out_features=1)
        self.fc_a = nn.Linear(in_features=vertex_num, out_features=1)
        self.fc_v = nn.Linear(in_features=vertex_num, out_features=1)
        self.fc_m = nn.Linear(in_features=unified_size * 3, out_features=unified_size)

        # Prediction
        self.fc_pre = nn.Linear(in_features=unified_size, out_features=label_dim)
        self.trm = nn.TransformerEncoderLayer(d_model=unified_size, nhead=2, batch_first=True)
        
        # å¯è§£é‡Šæ€§æ¨¡å—
        self.explainer = ExplainableDetection(unified_size, vertex_num)
        
        # ç¥ç»ç¬¦å·è§„åˆ™å¼•æ“
        self.neural_symbolic_engine = NeuralSymbolicRuleEngine()
        # æ˜¯å¦å¯ç”¨ç¥ç»ç¬¦å·è§„åˆ™
        self.enable_neural_symbolic = modelConfig.get("enable_neural_symbolic", True)
        # å­˜å‚¨è§„åˆ™é˜ˆå€¼ï¼Œé¿å…åœ¨forwardä¸­è®¿é—®modelConfig
        self.rule_threshold = modelConfig.get("rule_threshold", 0.1)
        
        # éšå¼æ„è§åˆ†æå™¨ï¼ˆå¯é€‰ï¼Œç”¨äºå®æ—¶åˆ†æï¼‰
        self.implicit_analyzer = None
        if modelConfig.get("enable_implicit_analysis", False):
            try:
                self.implicit_analyzer = ImplicitOpinionAnalyzer(
                    llm_model_name=modelConfig.get("llm_model_name", "THUDM/chatglm-6b")
                )
                print("éšå¼æ„è§åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"éšå¼æ„è§åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.implicit_analyzer = None
        
        # ä¿å­˜åŸå§‹å›¾åƒå°ºå¯¸ä»¥ä¾¿å¯è§†åŒ–ï¼ˆè®­ç»ƒä¸­è®¾ç½®ï¼‰
        self.original_video_frames = None
        # æ˜¯å¦å¯ç”¨å¯è§£é‡Šæ€§åŠŸèƒ½
        self.enable_explanation = modelConfig.get("enable_explanation", False)

    def forward(self, texts, audios, videos, comments, c3d, user_intro, gpt_description, return_explanations=False, implicit_opinion_data=None):
        # ä½¿ç”¨withè¯­å¥ç¡®ä¿ä¸´æ—¶å˜é‡è¢«é‡Šæ”¾
        with torch.set_grad_enabled(self.training):
            # Feature Extraction
            texts_local, _ = self.fc_pre_t_1(texts)
            texts_local = self.fc_pre_t_2(texts_local)
            
            # æ˜¾å¼é‡Šæ”¾ä¸å†éœ€è¦çš„å˜é‡
            del texts
            
            # è°ƒæ•´audiosçš„å½¢çŠ¶
            # æ‰“å°åŸå§‹å½¢çŠ¶ä»¥ä¾¿è°ƒè¯•
            original_shape = audios.shape
            print(f"åŸå§‹audioså½¢çŠ¶: {original_shape}")
            
            # æ ¹æ®è®ºæ–‡ä¸­VGGishæ¨¡å‹çš„è¦æ±‚è°ƒæ•´éŸ³é¢‘ç‰¹å¾
            # VGGishæœŸæœ›è¾“å…¥å½¢çŠ¶ä¸º[batch_size, 1, time_steps, freq_bins]
            try:
                # å¦‚æœæ˜¯4Då¼ é‡ï¼Œè°ƒæ•´é€šé“é¡ºåº
                if len(original_shape) == 4:
                    b, c, t, f = original_shape
                    # å¦‚æœé€šé“æ•°å¤§äº1ï¼Œåªä½¿ç”¨ç¬¬ä¸€ä¸ªé€šé“
                    if c > 1:
                        audios = audios[:, 0:1, :, :]
                        print(f"è°ƒæ•´åaudioså½¢çŠ¶ (é€‰æ‹©ç¬¬ä¸€ä¸ªé€šé“): {audios.shape}")
                # å¦‚æœæ˜¯3Då¼ é‡ï¼Œå¢åŠ é€šé“ç»´åº¦
                elif len(original_shape) == 3:
                    b, t, f = original_shape
                    audios = audios.unsqueeze(1)  # æ·»åŠ é€šé“ç»´åº¦
                    print(f"è°ƒæ•´åaudioså½¢çŠ¶ (å¢åŠ é€šé“ç»´åº¦): {audios.shape}")
                # å¦‚æœæ˜¯2Då¼ é‡ï¼Œè§†ä¸ºå•ä¸ªæ ·æœ¬ï¼Œæ·»åŠ batchå’Œé€šé“ç»´åº¦
                elif len(original_shape) == 2:
                    t, f = original_shape
                    audios = audios.unsqueeze(0).unsqueeze(1)  # æ·»åŠ batchå’Œé€šé“ç»´åº¦
                    print(f"è°ƒæ•´åaudioså½¢çŠ¶ (å¢åŠ batchå’Œé€šé“ç»´åº¦): {audios.shape}")
                
                # å°è¯•é€šè¿‡vggish_modifiedå¤„ç†
                try:
                    # é¦–å…ˆç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                    audios = audios.float()
                    audios = self.vggish_modified(audios)
                    print(f"vggish_modifiedå¤„ç†åaudioså½¢çŠ¶: {audios.shape}")
                    
                    # é‡å¡‘audiosåˆ°fc_pre_aæœŸæœ›çš„å½¢çŠ¶
                    # fc_pre_aåº”è¯¥æœŸæœ›è¾“å…¥ç»´åº¦ä¸º[batch_size, a_in]ï¼Œå…¶ä¸­a_inä¸º128
                    batch_size = audios.shape[0]
                    audios = audios.reshape(batch_size, -1)
                    print(f"é‡å¡‘åaudioså½¢çŠ¶: {audios.shape}")
                    
                    # å¦‚æœéœ€è¦ï¼Œæˆªæ–­æˆ–å¡«å……åˆ°a_inçš„å¤§å°
                    a_in_size = 128  # æ ¹æ®modelConfig["a_in"]å‚æ•°
                    if audios.shape[1] > a_in_size:
                        # æˆªæ–­åˆ°a_in_size
                        audios = audios[:, :a_in_size]
                        print(f"æˆªæ–­åaudioså½¢çŠ¶: {audios.shape}")
                    elif audios.shape[1] < a_in_size:
                        # å¡«å……åˆ°a_in_size
                        padding = torch.zeros(batch_size, a_in_size - audios.shape[1], device=audios.device)
                        audios = torch.cat([audios, padding], dim=1)
                        print(f"å¡«å……åaudioså½¢çŠ¶: {audios.shape}")
                    
                    # æ˜¾å¼æ¸…ç†ä¸´æ—¶å˜é‡ï¼Œå‡å°‘å†…å­˜å ç”¨
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                
                except RuntimeError as e:
                    print(f"vggish_modifiedå¤„ç†å¤±è´¥ï¼Œé”™è¯¯: {e}")
                    print(f"å°è¯•è°ƒæ•´å½¢çŠ¶åå†å¤„ç†...")
                    
                    # å°è¯•ä¸åŒçš„è°ƒæ•´æ–¹å¼
                    if len(audios.shape) == 4:
                        # å°è¯•ä¸åŒçš„è°ƒæ•´æ–¹å¼
                        b, c, t, f = audios.shape
                        
                        # æ–¹æ¡ˆ1: ç¡®ä¿åªæœ‰ä¸€ä¸ªé€šé“
                        if c > 1:
                            audios = audios[:, 0:1, :, :]
                        
                        # æ–¹æ¡ˆ2: å¦‚æœç‰¹å¾ç»´åº¦å¤ªå¤§ï¼Œå°è¯•å°†æ—¶é—´å’Œé¢‘ç‡ç»´åº¦å±•å¹³
                        if audios.shape[-1] * audios.shape[-2] > 128:
                            # é‡å¡‘ä¸º[batch_size, 1, time*freq]
                            audios = audios.view(b, 1, -1)
                            # æˆªå–åˆ°åˆé€‚çš„é•¿åº¦
                            audios = audios[:, :, :128]
                            print(f"è°ƒæ•´åaudioså½¢çŠ¶ (å±•å¹³å¹¶æˆªå–): {audios.shape}")
                            
                            # å¦‚æœvggishæœŸæœ›2Dè¾“å…¥ï¼Œå»æ‰é€šé“ç»´åº¦
                            audios = audios.squeeze(1)
                        
                    # å†æ¬¡å°è¯•
                    audios = self.vggish_modified(audios)
                    print(f"é‡æ–°è°ƒæ•´åï¼Œvggish_modifiedå¤„ç†æˆåŠŸï¼Œå½¢çŠ¶: {audios.shape}")
                    
                    # é‡å¡‘ä¸ºfc_pre_aæœŸæœ›çš„å½¢çŠ¶
                    batch_size = audios.shape[0]
                    audios = audios.reshape(batch_size, -1)
                    a_in_size = 128
                    if audios.shape[1] > a_in_size:
                        audios = audios[:, :a_in_size]
                        print(f"æˆªæ–­åaudioså½¢çŠ¶: {audios.shape}")
                    elif audios.shape[1] < a_in_size:
                        padding = torch.zeros(batch_size, a_in_size - audios.shape[1], device=audios.device)
                        audios = torch.cat([audios, padding], dim=1)
                        print(f"å¡«å……åaudioså½¢çŠ¶: {audios.shape}")
                    
            except Exception as e:
                print(f"å¤„ç†éŸ³é¢‘æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {e}")
                # åœ¨é”™è¯¯æƒ…å†µä¸‹ï¼Œåˆ›å»ºä¸€ä¸ªä¸é¢„æœŸè¾“å‡ºå½¢çŠ¶åŒ¹é…çš„é›¶å¼ é‡
                # æ ¹æ®fc_pre_açš„è¾“å…¥ç»´åº¦ç¡®å®šå½¢çŠ¶
                batch_size = original_shape[0] if len(original_shape) >= 1 else 1
                audios = torch.zeros(batch_size, 128, device=texts_local.device)
                print(f"ä½¿ç”¨é›¶å¼ é‡æ›¿ä»£ï¼Œå½¢çŠ¶: {audios.shape}")
            
            audios_local = self.fc_pre_a(audios)
            c3d_local = self.fc_pre_c3d(c3d)
            gpt_local, _ = self.fc_pre_gpt_1(gpt_description)
            gpt_local = self.fc_pre_gpt_2(gpt_local)
            comments_global = self.fc_pre_com(comments)
            user_intro_global = self.fc_pre_user(user_intro.squeeze())
            
            # æ£€æŸ¥å¹¶å¤„ç†è§†é¢‘ç‰¹å¾å¼ é‡
            original_videos_shape = videos.shape
            print(f"åŸå§‹videoså½¢çŠ¶: {original_videos_shape}")
            
            # ç¡®ä¿è§†é¢‘ç‰¹å¾ç»´åº¦æ­£ç¡®
            try:
                # æ£€æŸ¥videosæ˜¯å¦åªæœ‰2Dè€Œæ²¡æœ‰æ‰¹æ¬¡ç»´åº¦ (ç‰¹åˆ«æ˜¯éªŒè¯/æµ‹è¯•é˜¶æ®µå¯èƒ½å‡ºç°)
                if len(original_videos_shape) == 2:
                    # å¯¹äºå½¢çŠ¶ä¸º[83, 4096]çš„æƒ…å†µï¼Œéœ€è¦å¢åŠ æ‰¹æ¬¡ç»´åº¦
                    print(f"videosæ˜¯2Då¼ é‡ï¼Œç¼ºå°‘æ‰¹æ¬¡ç»´åº¦ã€‚æ·»åŠ æ‰¹æ¬¡ç»´åº¦")
                    
                    # æ£€æµ‹è¿™ä¸ªæ˜¯å¦æ˜¯å•æ ·æœ¬çš„æƒ…å†µ
                    if batch_size == 1 or texts_local.shape[0] == 1:
                        print(f"å•æ ·æœ¬æƒ…å†µä¸‹çš„videos 2Då¼ é‡å¤„ç†")
                        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦: [83, 4096] -> [1, 83, 4096]
                        videos = videos.unsqueeze(0)
                        print(f"ä¸ºvideosæ·»åŠ æ‰¹æ¬¡ç»´åº¦åå½¢çŠ¶: {videos.shape}")
                        # æ›´æ–°å½¢çŠ¶ä¿¡æ¯
                        original_videos_shape = videos.shape
                    else:
                        print(f"è­¦å‘Š: æ‰¹æ¬¡å¤§å°ä¸ä¸º1 ({batch_size}) ä½†videoså¼ é‡æ²¡æœ‰æ‰¹æ¬¡ç»´åº¦")
                        # å°è¯•æ‰©å±•æ‰¹æ¬¡ç»´åº¦, ä½†éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                        try:
                            # å°è¯•é€šè¿‡å¤åˆ¶æ‰©å±•æ‰¹æ¬¡ç»´åº¦
                            videos = videos.unsqueeze(0).expand(batch_size, *original_videos_shape)
                            print(f"æ‰©å±•åvideoså½¢çŠ¶: {videos.shape}")
                            original_videos_shape = videos.shape
                        except Exception as e:
                            print(f"æ‰©å±•videosæ‰¹æ¬¡ç»´åº¦å¤±è´¥: {e}")
                            # åˆ›å»ºä¸€ä¸ªç¬¦åˆé¢„æœŸå½¢çŠ¶çš„é›¶å¼ é‡
                            seq_len, features = original_videos_shape
                            videos = torch.zeros(batch_size, seq_len, features, device=videos.device)
                            print(f"ä½¿ç”¨é›¶å¼ é‡æ›¿ä»£ï¼Œå½¢çŠ¶: {videos.shape}")
                            original_videos_shape = videos.shape
                
                # å¦‚æœè§†é¢‘ç‰¹å¾æ˜¯3Då¼ é‡ [batch_size, seq_len, features]
                if len(original_videos_shape) == 3:
                    batch_size, seq_len, features = original_videos_shape
                    
                    # éªŒè¯ç‰¹å¾ç»´åº¦æ˜¯å¦ç¬¦åˆé¢„æœŸ
                    if features != 4096:  # v_iné¢„æœŸä¸º4096
                        print(f"è­¦å‘Š: è§†é¢‘ç‰¹å¾ç»´åº¦ä¸ç¬¦åˆé¢„æœŸï¼Œå½“å‰ä¸º{features}ï¼Œé¢„æœŸä¸º4096")
                        # å¦‚æœç‰¹å¾ç»´åº¦å¤ªå°æˆ–å¤ªå¤§ï¼Œè¿›è¡Œè°ƒæ•´
                        if features < 4096:
                            # æ‰©å±•ç‰¹å¾ç»´åº¦
                            padding = torch.zeros(batch_size, seq_len, 4096 - features, device=videos.device)
                            videos = torch.cat([videos, padding], dim=2)
                        else:
                            # æˆªæ–­ç‰¹å¾ç»´åº¦
                            videos = videos[:, :, :4096]
                        print(f"è°ƒæ•´åvideoså½¢çŠ¶: {videos.shape}")
                    
                    # ç›´æ¥åº”ç”¨fc_pre_v
                    videos = self.fc_pre_v(videos)  # ä»[batch, seq_len, 4096]åˆ°[batch, seq_len, v_in_pre]
                    print(f"fc_pre_vå¤„ç†åvideoså½¢çŠ¶: {videos.shape}")
                    
                    # è®¡ç®—å…¨å±€ç‰¹å¾ï¼Œå–å¹³å‡å€¼
                    videos_global = torch.mean(videos, dim=1)  # [batch, v_in_pre]
                    print(f"è§†é¢‘å…¨å±€ç‰¹å¾å½¢çŠ¶: {videos_global.shape}")
                
                # å¦‚æœè§†é¢‘ç‰¹å¾æ˜¯2Då¼ é‡ [batch_size, features] - å¯èƒ½å·²ç»æ˜¯å…¨å±€ç‰¹å¾
                elif len(original_videos_shape) == 2 and original_videos_shape[0] == batch_size:
                    print("è§†é¢‘ç‰¹å¾ä¸º2Då¼ é‡ [batch_size, features]ï¼Œå¯èƒ½å·²ç»æ˜¯å…¨å±€ç‰¹å¾")
                    batch_size, features = original_videos_shape
                    
                    # å¦‚æœæ˜¯å…¨å±€ç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶ä¸ºlocalå’Œglobalå¤„ç†
                    if features == self.fc_pre_v.in_features:  # æ£€æŸ¥ç‰¹å¾ç»´åº¦æ˜¯å¦ç¬¦åˆfc_pre_vçš„è¾“å…¥ç»´åº¦
                        # æ·»åŠ ä¸€ä¸ªåºåˆ—é•¿åº¦ç»´åº¦
                        temp_videos = videos.unsqueeze(1)  # [batch, 1, features]
                        videos = self.fc_pre_v(temp_videos)  # [batch, 1, v_in_pre]
                        videos_global = videos.squeeze(1)  # å»æ‰æ—¶åºç»´åº¦ [batch, v_in_pre]
                    else:
                        # ç‰¹å¾ç»´åº¦ä¸åŒ¹é…ï¼Œéœ€è¦è°ƒæ•´
                        print(f"è­¦å‘Š: è§†é¢‘ç‰¹å¾ç»´åº¦ä¸ç¬¦åˆé¢„æœŸï¼Œå½“å‰ä¸º{features}ï¼Œé¢„æœŸä¸º{self.fc_pre_v.in_features}")
                        
                        # æ·»åŠ åºåˆ—é•¿åº¦ç»´åº¦
                        videos = videos.unsqueeze(1)  # [batch, 1, features]
                        
                        # è°ƒæ•´ç‰¹å¾ç»´åº¦
                        if features < self.fc_pre_v.in_features:
                            # æ‰©å±•ç‰¹å¾ç»´åº¦
                            padding = torch.zeros(batch_size, 1, self.fc_pre_v.in_features - features, device=videos.device)
                            videos = torch.cat([videos, padding], dim=2)
                        else:
                            # æˆªæ–­ç‰¹å¾ç»´åº¦
                            videos = videos[:, :, :self.fc_pre_v.in_features]
                        
                        videos = self.fc_pre_v(videos)  # [batch, 1, v_in_pre]
                        videos_global = videos.squeeze(1)  # [batch, v_in_pre]
                
                else:
                    raise ValueError(f"è§†é¢‘ç‰¹å¾ç»´åº¦å¼‚å¸¸: {original_videos_shape}")
                
            except Exception as e:
                print(f"å¤„ç†è§†é¢‘ç‰¹å¾æ—¶å‡ºé”™: {e}")
                # å‡ºé”™æ—¶åˆ›å»ºé›¶å¼ é‡æ›¿ä»£
                batch_size = texts_local.shape[0] if hasattr(texts_local, 'shape') and len(texts_local.shape) > 0 else 1
                videos = torch.zeros(batch_size, 83, 1000, device=comments_global.device)  # v_in_pre=1000
                videos_global = torch.zeros(batch_size, 1000, device=comments_global.device)
                print(f"ä½¿ç”¨é›¶å¼ é‡æ›¿ä»£ï¼Œå½¢çŠ¶: videos={videos.shape}, videos_global={videos_global.shape}")
            
            # Intra-modal Enhancement
            z_t, z_g, z_a, z_v = self.CrossmodalTransformer(texts_local, gpt_local, audios_local, videos)  # (49,32,64) (200,32,64)
            z_t = self.fc_g_t(torch.cat([z_t, z_g], dim=2))
            z_a = self.fc_a_MTout(z_a)
            z_v = self.fc_v_MTout(z_v)
            
            # æ‰“å°ç‰¹å¾å½¢çŠ¶ï¼Œç”¨äºè°ƒè¯•
            print(f"å¢å¼ºåç‰¹å¾å½¢çŠ¶: z_t={z_t.shape}, z_a={z_a.shape}, z_v={z_v.shape}")
            
            # æ£€æŸ¥ç‰¹å¾ç»´åº¦æ˜¯å¦ä¸€è‡´ï¼Œå¿…è¦æ—¶è¿›è¡Œè°ƒæ•´
            # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åœ¨ç¬¬ä¸€ç»´ï¼ˆåºåˆ—é•¿åº¦ï¼‰ä¸Šå…·æœ‰ç›¸åŒçš„ç»´åº¦
            target_seq_len = min(z_t.shape[0], z_a.shape[0], z_v.shape[0])
            if z_t.shape[0] != target_seq_len:
                print(f"è°ƒæ•´z_tåºåˆ—é•¿åº¦ä»{z_t.shape[0]}åˆ°{target_seq_len}")
                z_t = z_t[:target_seq_len]
            if z_a.shape[0] != target_seq_len:
                print(f"è°ƒæ•´z_aåºåˆ—é•¿åº¦ä»{z_a.shape[0]}åˆ°{target_seq_len}")
                z_a = z_a[:target_seq_len]
            if z_v.shape[0] != target_seq_len:
                print(f"è°ƒæ•´z_våºåˆ—é•¿åº¦ä»{z_v.shape[0]}åˆ°{target_seq_len}")
                z_v = z_v[:target_seq_len]
            
            # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åœ¨æœ€åä¸€ç»´ï¼ˆç‰¹å¾ç»´åº¦ï¼‰ä¸Šå…·æœ‰ç›¸åŒçš„ç»´åº¦
            # è¿™é‡Œå‡è®¾StoGæœŸæœ›æ‰€æœ‰ç‰¹å¾å…·æœ‰ç›¸åŒçš„ç»´åº¦
            # å¦‚æœä¸åŒï¼Œå¯èƒ½éœ€è¦å…ˆé€šè¿‡çº¿æ€§å±‚è°ƒæ•´
            
            print(f"è°ƒæ•´åç‰¹å¾å½¢çŠ¶: z_t={z_t.shape}, z_a={z_a.shape}, z_v={z_v.shape}")
            
            try:
                x_t, x_a, x_v = self.StoG(z_t, z_a, z_v, self.batch_size) #(32,32,64)
                print(f"StoGè¾“å‡ºå½¢çŠ¶: x_t={x_t.shape}, x_a={x_a.shape}, x_v={x_v.shape}")
            except RuntimeError as e:
                print(f"StoGå¤„ç†é”™è¯¯: {e}")
                # å‡ºé”™æ—¶ï¼Œå°è¯•è°ƒæ•´ç‰¹å¾ç»´åº¦åé‡è¯•
                # 1. ç¡®ä¿æ‰€æœ‰ç‰¹å¾å…·æœ‰ç›¸åŒçš„å½¢çŠ¶
                batch_dim = z_t.shape[1]  # æ‰¹æ¬¡å¤§å°
                feature_dim = z_t.shape[2]  # ç‰¹å¾ç»´åº¦
                
                # æ‰“å°å½“å‰æ‰¹æ¬¡å¤§å°ä¿¡æ¯
                print(f"å½“å‰æ‰¹æ¬¡å¤§å°: {batch_dim}, é…ç½®çš„æ‰¹æ¬¡å¤§å°: {self.batch_size}")
                
                # å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º1ï¼Œä½¿ç”¨ç‰¹æ®Šå¤„ç†
                if batch_dim == 1:
                    print("æ£€æµ‹åˆ°æ‰¹æ¬¡å¤§å°ä¸º1ï¼Œä½¿ç”¨ç‰¹æ®Šå¤„ç†")
                    
                    # ä½¿ç”¨ä¸´æ—¶çš„æ‰¹æ¬¡å¤§å°
                    temp_batch_size = batch_dim
                    
                    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾é•¿åº¦ç›¸åŒä¸”éƒ½æ˜¯1
                    seq_len = z_t.shape[0]
                    if z_a.shape[0] != seq_len or z_v.shape[0] != seq_len:
                        print(f"è°ƒæ•´åºåˆ—é•¿åº¦: z_a={z_a.shape[0]}â†’{seq_len}, z_v={z_v.shape[0]}â†’{seq_len}")
                        
                        # å¦‚æœåºåˆ—é•¿åº¦ä¸åŒï¼Œåˆ›å»ºç›¸åŒé•¿åº¦çš„å¼ é‡
                        if z_a.shape[0] != seq_len:
                            new_z_a = torch.zeros_like(z_t)
                            min_len = min(z_a.shape[0], seq_len)
                            new_z_a[:min_len] = z_a[:min_len]
                            z_a = new_z_a
                        
                        if z_v.shape[0] != seq_len:
                            new_z_v = torch.zeros_like(z_t)
                            min_len = min(z_v.shape[0], seq_len)
                            new_z_v[:min_len] = z_v[:min_len]
                            z_v = new_z_v
                    
                    # å†æ¬¡å°è¯•ï¼Œä½¿ç”¨å®é™…æ‰¹æ¬¡å¤§å°
                    try:
                        x_t, x_a, x_v = self.StoG(z_t, z_a, z_v, temp_batch_size)
                        print(f"å°æ‰¹æ¬¡ç‰¹æ®Šå¤„ç†åStoGè¾“å‡ºå½¢çŠ¶: x_t={x_t.shape}, x_a={x_a.shape}, x_v={x_v.shape}")
                    except RuntimeError as e2:
                        print(f"å°æ‰¹æ¬¡ç‰¹æ®Šå¤„ç†ä»ç„¶å¤±è´¥: {e2}")
                        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œåˆ›å»ºè™šæ‹Ÿè¾“å‡º
                        # å‡è®¾StoGè¾“å‡ºç»´åº¦ä¸º[batch_size, vertex_num, feature_dim]
                        vertex_num = 32  # æ ¹æ®æ¨¡å‹é…ç½®
                        x_t = torch.zeros(batch_dim, vertex_num, feature_dim, device=z_t.device)
                        x_a = torch.zeros(batch_dim, vertex_num, feature_dim, device=z_a.device)
                        x_v = torch.zeros(batch_dim, vertex_num, feature_dim, device=z_v.device)
                        print(f"åˆ›å»ºè™šæ‹Ÿè¾“å‡º: å½¢çŠ¶={x_t.shape}")
                else:
                    # ä½¿ç”¨z_tä½œä¸ºæ¨¡æ¿ï¼Œå°†z_aå’Œz_vè°ƒæ•´ä¸ºç›¸åŒå½¢çŠ¶
                    if z_a.shape != z_t.shape:
                        print(f"è°ƒæ•´z_aå½¢çŠ¶ä»{z_a.shape}åˆ°{z_t.shape}")
                        z_a_new = torch.zeros_like(z_t)
                        min_seq = min(z_a.shape[0], z_t.shape[0])
                        min_batch = min(z_a.shape[1], z_t.shape[1])
                        min_feat = min(z_a.shape[2], z_t.shape[2])
                        z_a_new[:min_seq, :min_batch, :min_feat] = z_a[:min_seq, :min_batch, :min_feat]
                        z_a = z_a_new
                    
                    if z_v.shape != z_t.shape:
                        print(f"è°ƒæ•´z_vå½¢çŠ¶ä»{z_v.shape}åˆ°{z_t.shape}")
                        z_v_new = torch.zeros_like(z_t)
                        min_seq = min(z_v.shape[0], z_t.shape[0])
                        min_batch = min(z_v.shape[1], z_t.shape[1])
                        min_feat = min(z_v.shape[2], z_t.shape[2])
                        z_v_new[:min_seq, :min_batch, :min_feat] = z_v[:min_seq, :min_batch, :min_feat]
                        z_v = z_v_new
                    
                    print(f"é‡è¯•StoGï¼Œè¾“å…¥å½¢çŠ¶: z_t={z_t.shape}, z_a={z_a.shape}, z_v={z_v.shape}")
                    x_t, x_a, x_v = self.StoG(z_t, z_a, z_v, self.batch_size)
                    print(f"é‡è¯•æˆåŠŸï¼ŒStoGè¾“å‡ºå½¢çŠ¶: x_t={x_t.shape}, x_a={x_a.shape}, x_v={x_v.shape}")

            # Cross-modal Interaction
            x_m = torch.concat([x_t.squeeze(), x_a.squeeze(), x_v.squeeze()], dim=2)
            x_m = self.fc_m(x_m)

            # ç¡®ä¿æ‰¹æ¬¡å¤§å°æ­£ç¡®ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ‰¹æ¬¡æ—¶
            actual_batch_size = x_t.shape[0]
            if actual_batch_size != self.batch_size:
                print(f"å®é™…æ‰¹æ¬¡å¤§å°({actual_batch_size})ä¸é…ç½®çš„æ‰¹æ¬¡å¤§å°({self.batch_size})ä¸ä¸€è‡´ï¼Œä½¿ç”¨å®é™…å€¼")
            
            # ä½¿ç”¨å®é™…æ‰¹æ¬¡å¤§å°ç”Ÿæˆæ—¶é—´æ­¥é•¿
            t_t = torch.randint(self.T, size=(actual_batch_size, ), device=x_t.device) # batchsize (0->T-1)
            noise_t = torch.randn_like(x_t)
            x_tmp_t = (
                extract(self.sqrt_alphas_bar, t_t, x_t.shape) * x_t +
                extract(self.sqrt_one_minus_alphas_bar, t_t, x_t.shape) * noise_t)

            t_a = torch.randint(self.T, size=(actual_batch_size,), device=x_a.device)
            noise_a = torch.randn_like(x_a)
            x_tmp_a = (
                    extract(self.sqrt_alphas_bar, t_a, x_a.shape) * x_a +
                    extract(self.sqrt_one_minus_alphas_bar, t_a, x_a.shape) * noise_a)

            t_v = torch.randint(self.T, size=(actual_batch_size,), device=x_v.device)
            noise_v = torch.randn_like(x_v)
            x_tmp_v = (
                    extract(self.sqrt_alphas_bar, t_v, x_v.shape) * x_v +
                    extract(self.sqrt_one_minus_alphas_bar, t_v, x_v.shape) * noise_v)

            # æ‰“å°æ‰©æ•£å¤„ç†å‰å½¢çŠ¶
            print(f"æ‰©æ•£æ¨¡å‹è¾“å…¥å½¢çŠ¶: x_tmp_t={x_tmp_t.shape}, t_t={t_t.shape}, x_m={x_m.shape}")
            
            try:
                x_a_pre = self.model_a(x_tmp_a, t_a, x_m)
                x_v_pre = self.model_v(x_tmp_v, t_v, x_m)
                x_t_pre = self.model_t(x_tmp_t, t_t, x_m)
                
                # æ‰“å°é¢„æµ‹åå½¢çŠ¶
                print(f"æ‰©æ•£æ¨¡å‹è¾“å‡ºå½¢çŠ¶: x_t_pre={x_t_pre.shape}, x_a_pre={x_a_pre.shape}, x_v_pre={x_v_pre.shape}")
                
                # ç¡®ä¿é¢„æµ‹å¼ é‡ä¸åŸå§‹å¼ é‡å½¢çŠ¶ä¸€è‡´ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´MSEæŸå¤±è®¡ç®—é”™è¯¯
                if x_t_pre.shape != x_t.shape:
                    print(f"è­¦å‘Š: x_t_preå½¢çŠ¶({x_t_pre.shape})ä¸x_tå½¢çŠ¶({x_t.shape})ä¸åŒ¹é…")
                    # å¦‚æœåªæ˜¯batchç»´åº¦ä¸åŒï¼Œå¯ä»¥æˆªæ–­æˆ–è€…å¡«å……
                    if x_t_pre.shape[1:] == x_t.shape[1:]:
                        min_batch = min(x_t_pre.shape[0], x_t.shape[0])
                        x_t_pre = x_t_pre[:min_batch]
                        x_t = x_t[:min_batch]
                        print(f"è°ƒæ•´å: x_t_pre={x_t_pre.shape}, x_t={x_t.shape}")
                
                if x_a_pre.shape != x_a.shape:
                    print(f"è­¦å‘Š: x_a_preå½¢çŠ¶({x_a_pre.shape})ä¸x_aå½¢çŠ¶({x_a.shape})ä¸åŒ¹é…")
                    if x_a_pre.shape[1:] == x_a.shape[1:]:
                        min_batch = min(x_a_pre.shape[0], x_a.shape[0])
                        x_a_pre = x_a_pre[:min_batch]
                        x_a = x_a[:min_batch]
                        print(f"è°ƒæ•´å: x_a_pre={x_a_pre.shape}, x_a={x_a.shape}")
                
                if x_v_pre.shape != x_v.shape:
                    print(f"è­¦å‘Š: x_v_preå½¢çŠ¶({x_v_pre.shape})ä¸x_vå½¢çŠ¶({x_v.shape})ä¸åŒ¹é…")
                    if x_v_pre.shape[1:] == x_v.shape[1:]:
                        min_batch = min(x_v_pre.shape[0], x_v.shape[0])
                        x_v_pre = x_v_pre[:min_batch]
                        x_v = x_v[:min_batch]
                        print(f"è°ƒæ•´å: x_v_pre={x_v_pre.shape}, x_v={x_v.shape}")
                
                # ä¿®æ”¹è¿™é‡Œï¼šå°†MSEæŸå¤±è®¡ç®—çš„reductionä»'none'æ”¹ä¸º'mean'ï¼Œä½¿æŸå¤±å˜æˆæ ‡é‡å€¼
                loss_a = F.mse_loss(x_a_pre.squeeze(), x_a, reduction='mean')
                loss_t = F.mse_loss(x_t_pre.squeeze(), x_t, reduction='mean')
                loss_v = F.mse_loss(x_v_pre.squeeze(), x_v, reduction='mean')
                loss = loss_t + loss_a + loss_v
                
            except RuntimeError as e:
                print(f"æ‰©æ•£æ¨¡å‹å¤„ç†é”™è¯¯: {e}")
                # å¦‚æœå‡ºé”™ï¼Œåˆ›å»ºé›¶æŸå¤±å’Œé¢„æµ‹å€¼
                print("åˆ›å»ºé›¶å¼ é‡æ›¿ä»£")
                
                # åˆ›å»ºä¸åŸå§‹å¼ é‡ç›¸åŒå½¢çŠ¶çš„é›¶å¼ é‡ä½œä¸ºé¢„æµ‹å€¼
                x_t_pre = torch.zeros_like(x_t)
                x_a_pre = torch.zeros_like(x_a)
                x_v_pre = torch.zeros_like(x_v)
                
                # åˆ›å»ºé›¶æŸå¤±
                loss = torch.zeros(1, device=x_t.device)

            output_a = self.fc_a(x_a_pre.transpose(2,1))
            output_t = self.fc_t(x_t_pre.transpose(2,1))
            output_v = self.fc_v(x_v_pre.transpose(2,1))
            output_a = output_a.transpose(2, 1)
            output_t = output_t.transpose(2, 1)
            output_v = output_v.transpose(2, 1)

            comments_global = comments_global.unsqueeze(1)
            
            # å¤„ç†videos_globalçš„ç»´åº¦é—®é¢˜
            print(f"æœ€ç»ˆç‰¹å¾å½¢çŠ¶: output_t={output_t.shape}, output_a={output_a.shape}, videos_global={videos_global.shape}")
            
            # è°ƒæ•´videos_globalçš„ç»´åº¦ä»1000åˆ°128ï¼Œä¸å…¶ä»–ç‰¹å¾ä¿æŒä¸€è‡´
            if videos_global.shape[1] != 128:
                print(f"è°ƒæ•´videos_globalç»´åº¦ä»{videos_global.shape[1]}åˆ°128")
                # ä½¿ç”¨é¢„å®šä¹‰çš„çº¿æ€§å±‚è°ƒæ•´ç»´åº¦
                videos_global = self.videos_global_proj(videos_global)
                print(f"è°ƒæ•´åvideos_globalå½¢çŠ¶: {videos_global.shape}")
            
            videos_global = videos_global.unsqueeze(1)
            user_intro_global = user_intro_global.unsqueeze(1)
            
            print(f"æ‹¼æ¥å‰æœ€ç»ˆå½¢çŠ¶: output_t={output_t.shape}, output_a={output_a.shape}, videos_global={videos_global.shape}, output_v={output_v.shape}, comments_global={comments_global.shape}")

            # Prediction
            output_m = torch.concat([output_t, output_a, videos_global, user_intro_global, output_v, comments_global], dim=1)
            output_m = self.trm(output_m)
            output_m = torch.mean(output_m, -2)
            
            # ä¿å­˜å¤šæ¨¡æ€èåˆç‰¹å¾ç”¨äºç¥ç»ç¬¦å·è§„åˆ™
            multimodal_features = output_m.clone()
            
            output_m = self.fc_pre(output_m.squeeze())
            
            # å­˜å‚¨åŸå§‹è§†é¢‘å¸§å’Œx_vç”¨äºå¯è§†åŒ–
            self.original_video_features = videos  # ç”¨äºä¿å­˜å½“å‰æ‰¹æ¬¡çš„è§†é¢‘ç‰¹å¾
            
            # åº”ç”¨ç¥ç»ç¬¦å·è§„åˆ™ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰éšå¼æ„è§æ•°æ®ï¼‰
            rule_info = None
            print(f"ğŸ”§ ç¥ç»ç¬¦å·æ£€æŸ¥: enable_neural_symbolic={self.enable_neural_symbolic}, implicit_opinion_data={implicit_opinion_data is not None}")
            if self.enable_neural_symbolic and implicit_opinion_data is not None:
                print(f"ğŸ§  ç¥ç»ç¬¦å·è§„åˆ™: æ”¶åˆ°éšå¼æ„è§æ•°æ®ï¼Œç±»å‹={type(implicit_opinion_data)}")
                try:
                    # å¤„ç†ä¸åŒç±»å‹çš„éšå¼æ„è§æ•°æ®
                    if isinstance(implicit_opinion_data, dict):
                        # å•ä¸ªæ ·æœ¬çš„å­—å…¸æ•°æ®
                        opinion_analysis = implicit_opinion_data
                        print(f"ğŸ“Š å¤„ç†å­—å…¸ç±»å‹æ•°æ®")
                    elif isinstance(implicit_opinion_data, list):
                        # æ‰¹æ¬¡æ•°æ®åˆ—è¡¨
                        opinion_analysis = implicit_opinion_data
                        valid_count = sum(1 for x in implicit_opinion_data if x is not None)
                        print(f"ğŸ“Š å¤„ç†åˆ—è¡¨ç±»å‹æ•°æ®ï¼Œæ‰¹æ¬¡å¤§å°: {len(implicit_opinion_data)}, æœ‰æ•ˆæ ·æœ¬: {valid_count}")
                    elif isinstance(implicit_opinion_data, str):
                        # åŸå§‹æ–‡æœ¬ï¼Œéœ€è¦å®æ—¶åˆ†æ
                        if self.implicit_analyzer is not None:
                            opinion_analysis = self.implicit_analyzer.analyze_implicit_opinion(implicit_opinion_data)
                            print(f"ğŸ“Š å¤„ç†å­—ç¬¦ä¸²ç±»å‹æ•°æ®ï¼Œå®æ—¶åˆ†æ")
                        else:
                            opinion_analysis = None
                            print(f"âš ï¸ å­—ç¬¦ä¸²æ•°æ®ä½†æ²¡æœ‰å®æ—¶åˆ†æå™¨")
                    else:
                        opinion_analysis = None
                        print(f"âš ï¸ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {type(implicit_opinion_data)}")
                    
                    if opinion_analysis is not None:
                        print(f"ğŸ” å¼€å§‹åº”ç”¨ç¥ç»ç¬¦å·è§„åˆ™...")
                        # åº”ç”¨ç¥ç»ç¬¦å·è§„åˆ™è°ƒæ•´
                        # ä½¿ç”¨æ–‡æœ¬ç‰¹å¾ä½œä¸ºè°ƒæ•´ç›®æ ‡
                        adjusted_text_features, adjusted_prediction, rule_info = self.neural_symbolic_engine(
                            text_features=x_t,
                            model_prediction=torch.softmax(output_m, dim=-1),
                            implicit_opinion_analysis=opinion_analysis
                        )
                        
                        print(f"ğŸ” è§„åˆ™åº”ç”¨ç»“æœ: {rule_info}")
                        
                        # å¦‚æœè§„åˆ™å¼•æ“äº§ç”Ÿäº†æ˜¾è‘—è°ƒæ•´ï¼Œæ›´æ–°æœ€ç»ˆé¢„æµ‹
                        if rule_info and abs(rule_info.get("bias_adjustment", 0)) > self.rule_threshold:
                            output_m = torch.log(adjusted_prediction + 1e-8)  # è½¬å›logits
                            print(f"âœ… ç¥ç»ç¬¦å·è§„åˆ™è°ƒæ•´: æƒé‡è°ƒæ•´={rule_info.get('weight_adjustment', 0):.3f}, "
                                  f"åç½®è°ƒæ•´={rule_info.get('bias_adjustment', 0):.3f}")
                        else:
                            bias_adj = rule_info.get('bias_adjustment', 0) if rule_info else 0
                            print(f"âš ï¸ è§„åˆ™è°ƒæ•´å¹…åº¦è¿‡å°ï¼Œä¸æ›´æ–°é¢„æµ‹ã€‚åç½®è°ƒæ•´: {bias_adj:.6f}, é˜ˆå€¼: {self.rule_threshold}")
                    else:
                        print(f"âŒ opinion_analysis ä¸º Noneï¼Œè·³è¿‡è§„åˆ™åº”ç”¨")
                            
                except Exception as e:
                    print(f"ç¥ç»ç¬¦å·è§„åˆ™åº”ç”¨å¤±è´¥: {e}")
                    rule_info = {"error": str(e)}
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿”å›å¯è§£é‡Šæ€§ç»“æœ
            enable_explanation = self.enable_explanation or return_explanations
            
            # è¿”å›å¯è§£é‡Šæ€§ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
            if enable_explanation:
                # è°ƒç”¨å¯è§£é‡Šæ€§æ¨¡å—ç”Ÿæˆè§£é‡Š
                explanation = self.explainer(x_t, x_a, x_v, output_m)
                
                # æ·»åŠ ç‰¹å¾åˆ°è§£é‡Šç»“æœä¸­ï¼Œä»¥ä¾›å¯¹æ¯”å­¦ä¹ å’Œå¯¹æŠ—éªŒè¯ä½¿ç”¨
                explanation['text_features'] = x_t  # æ·»åŠ æ–‡æœ¬ç‰¹å¾
                explanation['audio_features'] = x_a  # æ·»åŠ éŸ³é¢‘ç‰¹å¾
                explanation['video_features'] = x_v  # æ·»åŠ è§†é¢‘ç‰¹å¾
                explanation['unified_features'] = output_m  # æ·»åŠ ç»Ÿä¸€ç‰¹å¾
                
                # æ·»åŠ ç¥ç»ç¬¦å·è§„åˆ™ä¿¡æ¯åˆ°è§£é‡Šä¸­
                if rule_info is not None:
                    explanation['neural_symbolic_rules'] = rule_info
                
                # å¦‚æœæ¨¡å‹é¢„æµ‹ä¸ºè™šå‡ï¼ˆç±»åˆ«1ï¼‰ï¼Œåˆ™åˆ›å»ºçƒ­å›¾
                # è·å–é¢„æµ‹çš„ç±»åˆ«
                _, predicted_class = torch.max(output_m, dim=1)
                
                # è®°å½•é¢„æµ‹ç»“æœåˆ°è§£é‡Šä¸­
                explanation['predicted_class'] = predicted_class
                
                return loss, output_m, explanation
            
            return loss, output_m
        
    def get_explanations(self, texts, audios, videos, comments, c3d, user_intro, gpt_description):
        """
        ä¸“é—¨ç”¨äºè·å–è§£é‡Šç»“æœçš„æ–¹æ³•ï¼Œä¸è¿›è¡Œè®­ç»ƒ
        è¿”å›æ¨¡å‹çš„é¢„æµ‹ç»“æœå’Œè§£é‡Šä¿¡æ¯
        """
        with torch.no_grad():
            loss, pred, explanation = self.forward(
                texts, audios, videos, comments, c3d, user_intro, gpt_description,
                return_explanations=True
            )
        return pred, explanation
    
    def visualize_fake_regions(self, explanation_dict, video_frames=None, save_path=None):
        """
        å¯è§†åŒ–è™šå‡åŒºåŸŸçš„æ–¹æ³•
        
        Args:
            explanation_dict: åŒ…å«å¯è§£é‡Šæ€§ä¿¡æ¯çš„å­—å…¸
            video_frames: åŸå§‹è§†é¢‘å¸§ (å¦‚æœæœ‰)
            save_path: ä¿å­˜å¯è§†åŒ–ç»“æœçš„è·¯å¾„
            
        Returns:
            å¯è§†åŒ–ç»“æœ
        """
        return self.explainer.visualize_explanation(
            explanation_dict, 
            video_frames if video_frames is not None else self.original_video_frames,
            save_path
        )