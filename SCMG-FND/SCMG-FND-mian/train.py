from typing import Dict
import torch
from tqdm import tqdm
import torch.nn.functional as F
from torch.utils.data import DataLoader
import random
import os
import numpy as np
import torch.optim as optim

# å¯¼å…¥æ–°å¢çš„å¤šç²’åº¦å¯¹æ¯”å­¦ä¹ å’Œå¯¹æŠ—æ€§éªŒè¯æ¨¡å—
from modules.MultiGranularityContrast import MultiGranularityContrast
from modules.AdversarialVerification import AdversarialVerification

def train(trainer, device, train_loader, val_loader, optimizer, epoch, 
        modelConfig, criterion=None, contrast_module=None, adv_framework=None):
    """
    è®­ç»ƒä¸€ä¸ªepoch
    
    Args:
        trainer: æ¨¡å‹
        device: è®¡ç®—è®¾å¤‡
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        optimizer: ä¼˜åŒ–å™¨
        epoch: å½“å‰epoch
        modelConfig: æ¨¡å‹é…ç½®å­—å…¸
        criterion: æŸå¤±å‡½æ•°
        contrast_module: å¤šç²’åº¦å¯¹æ¯”å­¦ä¹ æ¨¡å—
        adv_framework: å¯¹æŠ—æ€§éªŒè¯æ¡†æ¶
        
    Returns:
        train_loss: è®­ç»ƒæŸå¤±
        train_acc: è®­ç»ƒå‡†ç¡®ç‡
        valid_loss: éªŒè¯æŸå¤±
        valid_acc: éªŒè¯å‡†ç¡®ç‡
        explanations: å¯è§£é‡Šæ€§ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
    """
    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
    trainer.train()
    len_train = len(train_loader)
    
    # æ¢¯åº¦ç´¯ç§¯è®¾ç½®
    gradient_accumulation_steps = modelConfig.get('gradient_accumulation_steps', 1)
    effective_batch_size = modelConfig.get('batch_size', 32) * gradient_accumulation_steps
    print(f"ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯: æ­¥æ•°={gradient_accumulation_steps}, æœ‰æ•ˆæ‰¹é‡å¤§å°={effective_batch_size}")
    
    # åˆå§‹åŒ–æŒ‡æ ‡
    total_loss = 0
    total_acc = 0
    bsz_sum = 0
    diffusion_loss_sum = 0
    classification_loss_sum = 0
    explain_loss_sum = 0
    contrast_loss_sum = 0   # å¤šç²’åº¦å¯¹æ¯”æŸå¤±æ€»å’Œ
    adv_loss_sum = 0        # å¯¹æŠ—æ€§éªŒè¯æŸå¤±æ€»å’Œ
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥å¯¹æ¯”å­¦ä¹ æ¨¡å—ä½†é…ç½®ä¸­å¯ç”¨äº†ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å—
    if contrast_module is None and modelConfig.get("use_multi_granularity_contrast", False):
        print("åˆ›å»ºæ–°çš„å¤šç²’åº¦å¯¹æ¯”å­¦ä¹ æ¨¡å—...")
        contrast_module = MultiGranularityContrast(
            feature_dim=modelConfig["unified_size"],
            projection_dim=modelConfig.get("contrast_projection_dim", 64),
            temperature=modelConfig.get("contrast_temperature", 0.1),
            spatial_regions=modelConfig.get("contrast_spatial_regions", 4),
            temporal_segments=modelConfig.get("contrast_temporal_segments", 8),
            modal_components=3  # æ–‡æœ¬ã€éŸ³é¢‘ã€è§†é¢‘ä¸‰ç§æ¨¡æ€
        ).to(device)
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥å¯¹æŠ—æ€§éªŒè¯æ¡†æ¶ä½†é…ç½®ä¸­å¯ç”¨äº†ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„æ¡†æ¶
    if adv_framework is None and modelConfig.get("use_adversarial_verification", False):
        print("åˆ›å»ºæ–°çš„å¯¹æŠ—æ€§éªŒè¯æ¡†æ¶...")
        adv_framework = AdversarialVerification(
            feature_dim=modelConfig["unified_size"],
            hidden_dim=modelConfig.get("adv_hidden_dim", 256),
            z_dim=modelConfig.get("adv_z_dim", 64),
            num_layers=modelConfig.get("adv_num_layers", 2),
            dropout=modelConfig.get("adv_dropout", 0.3)
        ).to(device)
    
    # å®šä¹‰å¯¹æŠ—éªŒè¯çš„ä¼˜åŒ–å™¨
    adv_optimizer = None
    if adv_framework is not None:
        adv_optimizer = torch.optim.Adam([
            {'params': adv_framework.encoder.parameters()},
            {'params': adv_framework.generator.parameters()},
            {'params': adv_framework.discriminator.parameters(), 'lr': 1e-4}  # åˆ¤åˆ«å™¨ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
        ], lr=2e-4, betas=(0.5, 0.999))

    # åœ¨TQDMä¸­æ˜¾ç¤ºè¿›åº¦æ¡
    with tqdm(total=len_train, desc=f'Epoch {epoch + 1}/{modelConfig["epoch"]}', unit='batch', ncols=100) as pbar:
        for i, batch_data in enumerate(train_loader):
            # è§£ææ‰¹æ¬¡æ•°æ®
            try:
                # æ ¹æ®æ•°æ®åŠ è½½å™¨çš„è¿”å›æ ¼å¼è§£ææ‰¹æ¬¡æ•°æ®
                if isinstance(batch_data, dict):
                    # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
                    texts = batch_data.get("text").float().to(device)
                    audios = batch_data.get("audioframes").float().to(device)
                    videos = batch_data.get("frames").float().to(device)
                    labels = batch_data.get("label").long().to(device)
                    comments = batch_data.get("comments", torch.zeros(1)).to(device)
                    c3d = batch_data.get("c3d", torch.zeros(1)).to(device)
                    user_intro = batch_data.get("user_intro", torch.zeros(1)).to(device)
                    gpt_description = batch_data.get("gpt_description", torch.zeros(1)).to(device)
                    implicit_opinion_data = batch_data.get('implicit_opinion_data')  # æ–°å¢ï¼šè·å–éšå¼æ„è§æ•°æ®
                    print(f"ğŸ” å­—å…¸æ‰¹æ¬¡ä¸­çš„éšå¼æ„è§æ•°æ®: å­˜åœ¨={implicit_opinion_data is not None}, éç©ºä¸ªæ•°={sum(1 for x in implicit_opinion_data if x is not None) if implicit_opinion_data else 0}")
                elif isinstance(batch_data, (list, tuple)) and len(batch_data) >= 4:
                    # å¦‚æœæ˜¯åˆ—è¡¨æˆ–å…ƒç»„æ ¼å¼
                    texts = batch_data[0].float().to(device)
                    audios = batch_data[1].float().to(device)
                    videos = batch_data[2].float().to(device)
                    labels = batch_data[3].long().to(device)
                    # å¦‚æœæœ‰æ›´å¤šå…ƒç´ ï¼Œç»§ç»­è§£åŒ…
                    comments = batch_data[4].to(device) if len(batch_data) > 4 else torch.zeros(1).to(device)
                    c3d = batch_data[5].to(device) if len(batch_data) > 5 else torch.zeros(1).to(device)
                    user_intro = batch_data[6].to(device) if len(batch_data) > 6 else torch.zeros(1).to(device)
                    gpt_description = batch_data[7].to(device) if len(batch_data) > 7 else torch.zeros(1).to(device)
                    implicit_opinion_data = batch_data[8] if len(batch_data) > 8 else None # æ–°å¢ï¼šè·å–éšå¼æ„è§æ•°æ®ï¼ˆä¿æŒå­—å…¸æ ¼å¼ï¼‰
                else:
                    raise ValueError(f"æ— æ³•è§£ææ‰¹æ¬¡æ•°æ®ï¼Œæ ¼å¼: {type(batch_data)}")
                
                # æ£€æŸ¥æ‰¹æ¬¡å¤§å°
                batch_size = labels.size(0)
            except Exception as e:
                print(f"è§£ææ‰¹æ¬¡æ•°æ®æ—¶å‡ºé”™: {e}")
                print(f"æ‰¹æ¬¡æ•°æ®ç±»å‹: {type(batch_data)}")
                continue
            
            # ä»…åœ¨æ¢¯åº¦ç´¯ç§¯çš„ç¬¬ä¸€æ­¥æˆ–ä¸ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ—¶æ¸…é›¶æ¢¯åº¦
            if i % gradient_accumulation_steps == 0:
                optimizer.zero_grad()
            
            # å…³é—­è§£é‡Šæ¨¡å—çš„æ¢¯åº¦è®¡ç®—ä»¥æé«˜è®­ç»ƒé€Ÿåº¦
            if hasattr(trainer, 'explainer') and epoch < modelConfig.get('explain_start_epoch', 5):
                for param in trainer.explainer.parameters():
                    param.requires_grad = False
            elif hasattr(trainer, 'explainer'):
                for param in trainer.explainer.parameters():
                    param.requires_grad = True
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            use_amp = modelConfig.get('mixed_precision', False) and torch.cuda.is_available()
            with torch.cuda.amp.autocast(enabled=use_amp):
                # åˆ¤æ–­æ˜¯å¦éœ€è¦ç‰¹å¾ç”¨äºå¯¹æ¯”å­¦ä¹ æˆ–å¯¹æŠ—éªŒè¯
                need_features_for_contrast = contrast_module is not None and epoch >= modelConfig.get('contrast_start_epoch', 3)
                need_features_for_adv = adv_framework is not None and adv_optimizer is not None and epoch >= modelConfig.get('adv_start_epoch', 5)
                
                # å½“éœ€è¦è§£é‡Šæ€§åˆ†ææˆ–å¯¹æ¯”å­¦ä¹ æˆ–å¯¹æŠ—éªŒè¯æ—¶ï¼Œå¯ç”¨return_explanations
                if (modelConfig.get("enable_explanation", False) and epoch >= modelConfig.get('explain_start_epoch', 5)) or need_features_for_contrast or need_features_for_adv:
                    # å¸¦è§£é‡Šçš„å‰å‘ä¼ æ’­
                    loss, outputs, explanation_results = trainer(texts, audios, videos, comments, c3d, user_intro, gpt_description, return_explanations=True, implicit_opinion_data=implicit_opinion_data)
                    # æå–ç‰¹å¾ç”¨äºå¯¹æ¯”å­¦ä¹ å’Œå¯¹æŠ—éªŒè¯
                    unified_features = explanation_results.get('unified_features', None)
                    text_features = explanation_results.get('text_features', None)
                    audio_features = explanation_results.get('audio_features', None)
                    video_features = explanation_results.get('video_features', None)
                else:
                    # ä¸éœ€è¦è§£é‡Šæ€§ç»“æœå’Œç‰¹å¾
                    loss, outputs, *_ = trainer(texts, audios, videos, comments, c3d, user_intro, gpt_description, return_explanations=False, implicit_opinion_data=implicit_opinion_data)
                    unified_features, text_features, audio_features, video_features = None, None, None, None
                
                # è®¡ç®—ä¸»è¦æ‰©æ•£æŸå¤±
                diffusion_loss = loss * modelConfig.get('diffusion_loss_weight', 0.008)
                
                # è®¡ç®—åˆ†ç±»æŸå¤±
                classification_loss = torch.tensor(0.0, device=device)
                if criterion is not None:
                    classification_loss = criterion(outputs, labels)
                else:
                    classification_loss = F.cross_entropy(outputs, labels)
                
                # è®¡ç®—è§£é‡ŠæŸå¤±
                explain_loss = torch.tensor(0.0, device=device)
                if modelConfig.get("enable_explanation", False) and epoch >= modelConfig.get('explain_start_epoch', 5):
                    explain_loss = explanation_results.get('explain_loss', torch.tensor(0.0, device=device))
                
                # è®¡ç®—å¤šç²’åº¦å¯¹æ¯”å­¦ä¹ æŸå¤±
                contrast_loss = torch.tensor(0.0, device=device)
                if contrast_module is not None and epoch >= modelConfig.get('contrast_start_epoch', 3):
                    # ç¡®ä¿æœ‰å¿…è¦çš„ç‰¹å¾
                    if text_features is not None and audio_features is not None and video_features is not None and unified_features is not None:
                        contrast_loss = contrast_module(
                            text_features=text_features,
                            audio_features=audio_features,
                            video_features=video_features,
                            global_features=unified_features,
                            labels=labels
                        )
                    else:
                        print("è­¦å‘Š: ç¼ºå°‘å¯¹æ¯”å­¦ä¹ æ‰€éœ€çš„ç‰¹å¾")
                
                # è®¡ç®—å¯¹æŠ—æ€§éªŒè¯æŸå¤±
                adv_loss = torch.tensor(0.0, device=device)
                if adv_framework is not None and adv_optimizer is not None and epoch >= modelConfig.get('adv_start_epoch', 5):
                    if unified_features is not None:
                        # è®­ç»ƒåˆ¤åˆ«å™¨
                        adv_optimizer.zero_grad()
                        real_score, fake_score, _ = adv_framework.forward_discriminator(unified_features.detach())
                        d_loss = adv_framework.compute_discriminator_loss(real_score, fake_score)
                        d_loss.backward()
                        adv_optimizer.step()
                        
                        # è®­ç»ƒç”Ÿæˆå™¨
                        adv_optimizer.zero_grad()
                        gen_score, _, _ = adv_framework.forward_generator(unified_features.detach())
                        g_loss = adv_framework.compute_generator_loss(gen_score)
                        
                        # è®¡ç®—å¯¹æŠ—éªŒè¯æŸå¤±
                        adv_loss = g_loss * modelConfig.get('adv_weight', 0.1)
                    else:
                        print("è­¦å‘Š: ç¼ºå°‘å¯¹æŠ—éªŒè¯æ‰€éœ€çš„ç‰¹å¾")
                
                # ç»„åˆæ‰€æœ‰æŸå¤±
                total_loss_batch = classification_loss + diffusion_loss + \
                                   modelConfig.get('explain_weight', 0.1) * explain_loss + \
                                   modelConfig.get('contrast_weight', 0.1) * contrast_loss + \
                                   modelConfig.get('adv_weight', 0.1) * adv_loss
                
                # å¦‚æœä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œåˆ™å°†æŸå¤±é™¤ä»¥ç´¯ç§¯æ­¥æ•°
                if gradient_accumulation_steps > 1:
                    total_loss_batch = total_loss_batch / gradient_accumulation_steps
            
            # åå‘ä¼ æ’­
            total_loss_batch.backward()
            
            # åªåœ¨ç´¯ç§¯å®Œæˆåæ›´æ–°æƒé‡
            if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len_train:
                # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                if modelConfig.get('grad_clip', 0) > 0:
                    torch.nn.utils.clip_grad_norm_(trainer.parameters(), modelConfig.get('grad_clip', 1.0))
                
                optimizer.step()
                optimizer.zero_grad()  # ç¡®ä¿æ¢¯åº¦æ¸…é›¶
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs.data, 1)
            total = labels.size(0)
            correct = (predicted == labels).sum().item()
            accuracy = correct / total
            
            # ç´¯åŠ æŒ‡æ ‡
            total_loss += total_loss_batch.item() * labels.size(0) * (1 if gradient_accumulation_steps <= 1 else gradient_accumulation_steps)
            total_acc += accuracy * labels.size(0)
            bsz_sum += labels.size(0)
            diffusion_loss_sum += diffusion_loss.item() * labels.size(0)
            classification_loss_sum += classification_loss.item() * labels.size(0)
            explain_loss_sum += explain_loss.item() * labels.size(0)
            contrast_loss_sum += contrast_loss.item() * labels.size(0)
            adv_loss_sum += adv_loss.item() * labels.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
            pbar.set_postfix({
                'loss': total_loss / max(1, bsz_sum),
                'acc': total_acc / max(1, bsz_sum),
                'clf_loss': classification_loss_sum / max(1, bsz_sum),
                'diff_loss': diffusion_loss_sum / max(1, bsz_sum),
                'expl_loss': explain_loss_sum / max(1, bsz_sum),
                'cont_loss': contrast_loss_sum / max(1, bsz_sum),
                'adv_loss': adv_loss_sum / max(1, bsz_sum)
            })
            pbar.update(1)
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    train_loss = total_loss / max(1, bsz_sum)
    train_acc = total_acc / max(1, bsz_sum)
    
    # éªŒè¯
    print("å¼€å§‹éªŒè¯...")
    if criterion is None:
        criterion = torch.nn.CrossEntropyLoss()
    
    # ä½¿ç”¨validå‡½æ•°è¿›è¡ŒéªŒè¯
    if modelConfig.get("enable_explanation", False):
        valid_loss, valid_results, valid_truths, explanations = valid(val_loader, trainer, criterion, modelConfig)
    else:
        valid_loss, valid_results, valid_truths, explanations = valid(val_loader, trainer, criterion, modelConfig)
    
    # è®¡ç®—éªŒè¯å‡†ç¡®ç‡
    valid_acc = 0.0
    if len(valid_results) > 0 and len(valid_truths) > 0:
        valid_acc = (valid_results == valid_truths).float().mean().item()
    
    # æ‰“å°ç»“æœ
    print(f"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}")
    
    # è¿”å›è®­ç»ƒå’ŒéªŒè¯ç»“æœ - å§‹ç»ˆè¿”å›5ä¸ªå€¼ï¼ŒåŒ…æ‹¬explanationsï¼ˆå³ä½¿æ˜¯Noneï¼‰
    return train_loss, train_acc, valid_loss, valid_acc, explanations

def calculate_f1(y_pred, y_true):
    """è®¡ç®—F1åˆ†æ•°"""
    from sklearn.metrics import f1_score
    return f1_score(y_true, y_pred, average='macro')

def calculate_auc(y_pred, y_true):
    """è®¡ç®—AUCåˆ†æ•°"""
    from sklearn.metrics import roc_auc_score
    try:
        # å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œè½¬æ¢é¢„æµ‹å€¼ä¸ºæ¦‚ç‡å†è®¡ç®—AUC
        return roc_auc_score(y_true, y_pred)
    except:
        # å¯¹äºå¤šåˆ†ç±»é—®é¢˜æˆ–æ ¼å¼ä¸åŒ¹é…çš„æƒ…å†µï¼Œè¿”å›0
        return 0.0

def save_explanations(explanations, checkpoint_path, epoch):
    """ä¿å­˜è§£é‡Šç»“æœ"""
    import json
    
    explanation_dir = os.path.join(checkpoint_path, 'explanations')
    os.makedirs(explanation_dir, exist_ok=True)
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(os.path.join(explanation_dir, f'explanations_epoch_{epoch+1}.json'), 'w') as f:
        json.dump(explanations, f, indent=2)
    
    print(f"è§£é‡Šç»“æœå·²ä¿å­˜: explanations_epoch_{epoch+1}.json")

def valid(loader, trainer, criterion, modelConfig: Dict):
    trainer.eval()
    results = []
    truths = []
    total_loss = 0.0
    total_batch_size = 0
    
    # ç”¨äºæ”¶é›†å¯è§£é‡Šæ€§ç»“æœ
    explanations = [] if modelConfig.get("enable_explanation", False) else None
    
    # è·å–æ··åˆç²¾åº¦è®­ç»ƒçš„è®¾ç½®
    use_amp = modelConfig.get("use_amp", False) and torch.cuda.is_available()
    diffusion_loss_weight = modelConfig.get("diffusion_loss_weight", 0.008)
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(loader)):
            batch_size = batch["label"].size(0)
            
            # ç‰¹æ®Šå¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡ï¼ˆé€šå¸¸æ˜¯ä¸å®Œæ•´çš„ï¼‰
            is_last_batch = batch_idx == len(loader) - 1
            if is_last_batch and batch_size == 1:
                print(f"å¤„ç†éªŒè¯é›†ä¸­çš„æœ€åä¸€ä¸ªæ‰¹æ¬¡(æ‰¹æ¬¡å¤§å°={batch_size})ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†")
            
            # ç¡®ä¿batch_sizeä¸è¶…è¿‡æ¨¡å‹é…ç½®
            if batch_size != modelConfig["batch_size"]:
                print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡å¤§å° {batch_size} ä¸æ¨¡å‹é…ç½® {modelConfig['batch_size']} ä¸åŒ¹é…")
                
                # å°è¯•æ›´æ–°trainerä¸­çš„batch_sizeå‚æ•°
                if hasattr(trainer, 'batch_size'):
                    trainer.batch_size = batch_size
                    print(f"å·²æ›´æ–°trainer.batch_sizeä¸º {batch_size}")
                
            total_batch_size += batch_size
            
            # æå–å„æ¨¡æ€æ•°æ®
            texts = batch["text"]
            audios = batch["audioframes"]
            videos = batch["frames"]
            comments = batch["comments"]
            labels = batch["label"]
            c3d = batch["c3d"]
            user_intro = batch["user_intro"]
            gpt_description = batch["gpt_description"]
            implicit_opinion_data = batch.get('implicit_opinion_data')  # æ–°å¢ï¼šè·å–éšå¼æ„è§æ•°æ®
            
            # æ£€æŸ¥ç‰¹æ®Šæƒ…å†µï¼švideosæ˜¯äºŒç»´çš„ï¼ˆå¯èƒ½æ˜¯æœ€åä¸€ä¸ªæ‰¹æ¬¡ï¼‰
            if len(videos.shape) == 2:
                print(f"æ£€æµ‹åˆ°videosæ˜¯2Då¼ é‡: {videos.shape}ï¼Œå°è¯•è°ƒæ•´å½¢çŠ¶")
                try:
                    # å¦‚æœæ˜¯[seq_len, features]ï¼Œæ‰©å±•ä¸º[1, seq_len, features]
                    videos = videos.unsqueeze(0)
                    print(f"è°ƒæ•´åvideoså½¢çŠ¶: {videos.shape}")
                except Exception as e:
                    print(f"è°ƒæ•´videoså½¢çŠ¶å¤±è´¥: {e}")
            
            # æ£€æŸ¥ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœæ˜¯å•æ ·æœ¬ï¼Œç¡®ä¿æ‰€æœ‰ç‰¹å¾ç»´åº¦éƒ½æ­£ç¡®
            if batch_size == 1:
                print(f"æ‰¹æ¬¡å¤§å°ä¸º1ï¼Œç¡®ä¿æ‰€æœ‰ç‰¹å¾å½¢çŠ¶æ­£ç¡®")
                
                # ç¡®ä¿textsè‡³å°‘æ˜¯2D
                if len(texts.shape) == 1:
                    texts = texts.unsqueeze(0)
                    print(f"è°ƒæ•´åtextså½¢çŠ¶: {texts.shape}")
                
                # åŒæ ·å¤„ç†å…¶ä»–ç‰¹å¾
                if len(audios.shape) == 2:  # å¦‚æœæ˜¯[seq_len, features]
                    audios = audios.unsqueeze(0)
                    print(f"è°ƒæ•´åaudioså½¢çŠ¶: {audios.shape}")
            
            # ç§»åŠ¨åˆ°GPU
            if torch.cuda.is_available():
                audios = audios.cuda()
                texts = texts.cuda()
                videos = videos.cuda()
                comments = comments.cuda()
                labels = labels.cuda()
                c3d = c3d.cuda()
                user_intro = user_intro.cuda()
                gpt_description = gpt_description.cuda()

            try:
                # ä½¿ç”¨æ··åˆç²¾åº¦éªŒè¯
                if use_amp:
                    with torch.cuda.amp.autocast():
                        # æ ¹æ®æ˜¯å¦å¯ç”¨å¯è§£é‡Šæ€§é€‰æ‹©ä¸åŒçš„å‰å‘ä¼ æ’­æ–¹å¼
                        if modelConfig.get("enable_explanation", False):
                            loss, pred, explanation = trainer(texts, audios, videos, comments, c3d, user_intro, gpt_description, return_explanations=True, implicit_opinion_data=implicit_opinion_data)
                            # æ”¶é›†å¯è§£é‡Šæ€§ç»“æœ
                            if explanations is not None:
                                # ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ æ‰¹æ¬¡ç´¢å¼•
                                for i in range(batch_size):
                                    batch_explanation = {
                                        'batch_idx': batch_idx,
                                        'sample_idx': i,
                                        'label': labels[i].item(),
                                    }
                                    # å°†è§£é‡Šå­—å…¸ä¸­çš„å¼ é‡è½¬æ¢ä¸ºCPUä¸Šçš„NumPyæ•°ç»„
                                    for key, tensor in explanation.items():
                                        if isinstance(tensor, torch.Tensor):
                                            if tensor.dim() > 1 and i < tensor.shape[0]:
                                                batch_explanation[key] = tensor[i].detach().cpu().numpy()
                                    explanations.append(batch_explanation)
                        else:
                            loss, pred, *_ = trainer(texts, audios, videos, comments, c3d, user_intro, gpt_description, return_explanations=False, implicit_opinion_data=implicit_opinion_data)
                        
                        # è·å–é¢„æµ‹ç±»åˆ«
                        _, y = torch.max(pred, 1)
                        
                        # è®¡ç®—æŸå¤±
                        diffusion_loss = loss * diffusion_loss_weight
                else:
                    # å¸¸è§„éªŒè¯ï¼ˆæ— æ··åˆç²¾åº¦ï¼‰
                    # æ ¹æ®æ˜¯å¦å¯ç”¨å¯è§£é‡Šæ€§é€‰æ‹©ä¸åŒçš„å‰å‘ä¼ æ’­æ–¹å¼
                    if modelConfig.get("enable_explanation", False):
                        loss, pred, explanation = trainer(texts, audios, videos, comments, c3d, user_intro, gpt_description, return_explanations=True, implicit_opinion_data=implicit_opinion_data)
                        # æ”¶é›†å¯è§£é‡Šæ€§ç»“æœ
                        if explanations is not None:
                            # ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ æ‰¹æ¬¡ç´¢å¼•
                            for i in range(batch_size):
                                batch_explanation = {
                                    'batch_idx': batch_idx,
                                    'sample_idx': i,
                                    'label': labels[i].item(),
                                }
                                # å°†è§£é‡Šå­—å…¸ä¸­çš„å¼ é‡è½¬æ¢ä¸ºCPUä¸Šçš„NumPyæ•°ç»„
                                for key, tensor in explanation.items():
                                    if isinstance(tensor, torch.Tensor):
                                        if tensor.dim() > 1 and i < tensor.shape[0]:
                                            batch_explanation[key] = tensor[i].detach().cpu().numpy()
                                explanations.append(batch_explanation)
                    else:
                        loss, pred, *_ = trainer(texts, audios, videos, comments, c3d, user_intro, gpt_description, return_explanations=False, implicit_opinion_data=implicit_opinion_data)
                    
                    # è·å–é¢„æµ‹ç±»åˆ«
                    _, y = torch.max(pred, 1)
                    
                    # è®¡ç®—æŸå¤±
                    diffusion_loss = loss * diffusion_loss_weight
                
                # æ”¶é›†ç»“æœ
                results.append(y)
                truths.append(labels)
                total_loss += diffusion_loss
                
            except RuntimeError as e:
                print(f"éªŒè¯ä¸­å‡ºç°è¿è¡Œæ—¶é”™è¯¯: {e}")
                print(f"é”™è¯¯å‘ç”Ÿæ—¶çš„æ‰¹æ¬¡å¤§å°: {batch_size}")
                try:
                    print(f"å½¢çŠ¶ä¿¡æ¯: texts={texts.shape}, audios={audios.shape}, videos={videos.shape}, c3d={c3d.shape}")
                except:
                    print("æ— æ³•æ‰“å°å½¢çŠ¶ä¿¡æ¯")
                
                if batch_size == 1 and is_last_batch:
                    print("è¿™æ˜¯æ‰¹æ¬¡å¤§å°ä¸º1çš„æœ€åä¸€ä¸ªæ‰¹æ¬¡ï¼Œè·³è¿‡è€Œä¸ä¸­æ–­éªŒè¯è¿‡ç¨‹")
                    continue
                else:
                    # å¯¹äºå¤§æ‰¹æ¬¡ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†è¿™ä¸ªæ‰¹æ¬¡æˆ–ç»ˆæ­¢éªŒè¯
                    if modelConfig.get("skip_error_batches", True):
                        print("æ ¹æ®é…ç½®ï¼Œè·³è¿‡è¿™ä¸ªé”™è¯¯çš„æ‰¹æ¬¡")
                        continue
                    else:
                        print("æ ¹æ®é…ç½®ï¼Œä¸­æ–­éªŒè¯è¿‡ç¨‹")
                        break
            
            except Exception as e:
                print(f"éªŒè¯ä¸­å‡ºç°å…¶ä»–é”™è¯¯: {str(e)}")
                # é€šå¸¸è·³è¿‡è¿™ä¸ªæ‰¹æ¬¡
                if modelConfig.get("skip_error_batches", True):
                    continue
                else:
                    break

        # ç¡®ä¿æœ‰ç»“æœæ‰è¿›è¡Œè¿æ¥
        if results:
            try:
                # å°è¯•è¿æ¥æ‰€æœ‰ç»“æœ
                results = torch.cat(results)
                truths = torch.cat(truths)
                # å§‹ç»ˆè¿”å›4ä¸ªå€¼ï¼ŒåŒ…æ‹¬explanationsï¼ˆå³ä½¿æ˜¯Noneï¼‰
                return total_loss, results, truths, explanations
            except RuntimeError as e:
                print(f"è¿æ¥éªŒè¯ç»“æœæ—¶å‡ºé”™: {e}")
                # å°è¯•å¤„ç†ä¸åŒå¤§å°çš„ç»“æœå¼ é‡
                print("å°è¯•å¤„ç†ä¸åŒå¤§å°çš„ç»“æœå¼ é‡...")
                
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªéç©ºå¼ é‡çš„å½¢çŠ¶å’Œè®¾å¤‡
                first_result = None
                first_truth = None
                for r in results:
                    if r.numel() > 0:
                        first_result = r
                        break
                for t in truths:
                    if t.numel() > 0:
                        first_truth = t
                        break
                
                if first_result is not None and first_truth is not None:
                    # è°ƒæ•´æ‰€æœ‰å¼ é‡çš„å½¢çŠ¶
                    adjusted_results = []
                    adjusted_truths = []
                    
                    for r in results:
                        if r.numel() > 0:
                            adjusted_results.append(r)
                    
                    for t in truths:
                        if t.numel() > 0:
                            adjusted_truths.append(t)
                    
                    # å†æ¬¡å°è¯•è¿æ¥
                    try:
                        results = torch.cat(adjusted_results)
                        truths = torch.cat(adjusted_truths)
                        return total_loss, results, truths, explanations
                    except:
                        print("å¤„ç†åä»ç„¶æ— æ³•è¿æ¥ç»“æœï¼Œè¿”å›ç©ºç»“æœ")
        
        print("è­¦å‘Š: éªŒè¯è¿‡ç¨‹ä¸­æ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœ")
        # è¿”å›ç©ºç»“æœï¼Œä½†ä»ç„¶ä¿æŒ4ä¸ªè¿”å›å€¼
        return 0.0, torch.tensor([]), torch.tensor([]), explanations