import math
import os
import pickle
import json  # æ–°å¢ï¼šç”¨äºåŠ è½½éšå¼æ„è§æ•°æ®

import h5py
import jieba
import jieba.analyse as analyse
import numpy as np
import pandas as pd
import torch
from scipy.spatial import distance
from sklearn import preprocessing
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import TfidfVectorizer
from torch.utils.data import Dataset
import torch.nn as nn
from transformers import BertTokenizer
from torchvision import models
from transformers import BertModel, BertTokenizer
# from src.models import MULTModel
# from src.main import hyp_params
# avgpool = models.vgg19(pretrained=True).avgpool.cuda()
# classifier = models.vgg19(pretrained=True).classifier[:4].cuda()
import argparse
# from src.utils import *
from torch.utils.data import DataLoader
# from src import train


# å¾—åˆ°ä¸€ä¸ªè§†é¢‘å¯¹åº”çš„æ‰€æœ‰æ•°æ®
class SVFENDDataset(Dataset):

    def __init__(self, datamode='title+ocr', train_or_test='train', opinion_data_path='enhanced_results.json'):  #æ ‡é¢˜+è½¬å½•

        print(f"åˆå§‹åŒ–æ•°æ®é›†ï¼Œæ¨¡å¼: {datamode}, æ•°æ®é›†: {train_or_test}")
        
        # åŠ è½½éšå¼æ„è§æ•°æ®
        self.opinion_data = self.load_opinion_data(opinion_data_path)
        
        # è¯»å–å„æ¨¡æ€ç‰¹å¾
        #éŸ³é¢‘ç‰¹å¾vggish
        with open(os.path.join('data/audio', 'audio_'+train_or_test+'.pkl'), "rb") as fr:
            self.audio = pickle.load(fr)

        # æ–‡æœ¬ç‰¹å¾
        if datamode == 'title':
            with open(os.path.join('data/text_title_temporal', 'text_title_lhs_'+train_or_test+'.pkl'), "rb") as fr:
                self.text = pickle.load(fr)
        elif datamode == 'title+ocr':
            with open(os.path.join('data/text_title_ocr_temporal', 'text_title_ocr_lhs_'+train_or_test+'.pkl'), "rb") as fr:
                self.text = pickle.load(fr)
        elif datamode == 'both':
            # 'both' æ¨¡å¼ä½¿ç”¨ title+ocr æ•°æ®
            try:
                with open(os.path.join('data/text_title_ocr_temporal', 'text_title_ocr_lhs_'+train_or_test+'.pkl'), "rb") as fr:
                    self.text = pickle.load(fr)
            except FileNotFoundError:
                # å¦‚æœæ‰¾ä¸åˆ°title+ocrï¼Œå°è¯•title
                with open(os.path.join('data/text_title_temporal', 'text_title_lhs_'+train_or_test+'.pkl'), "rb") as fr:
                    self.text = pickle.load(fr)
        else:
            # é»˜è®¤ä½¿ç”¨title+ocr
            with open(os.path.join('data/text_title_ocr_temporal', 'text_title_ocr_lhs_'+train_or_test+'.pkl'), "rb") as fr:
                self.text = pickle.load(fr)

        with open(os.path.join('data/comments', 'comments_' + train_or_test + '.pkl'), "rb") as fr:
            self.comments = pickle.load(fr)

        # gptç”Ÿæˆçš„æ–‡æœ¬åˆ†æ
        with open(os.path.join('data/gpt_description', 'gpt_description_' + train_or_test + '.pkl'), "rb") as fr:
            self.gpt_description = pickle.load(fr)

        # label
        with open(os.path.join('data/label', 'label_'+train_or_test+'.pkl'), "rb") as fr:
            self.label = pickle.load(fr)

        #vgg9è§†é¢‘å¸§ç‰¹å¾
        with open(os.path.join('data/video', 'video_'+train_or_test+'.pkl'), "rb") as fr:
            self.video = pickle.load(fr)

        # user_intro
        with open(os.path.join('data/user_intro', 'user_intro_'+train_or_test+'.pkl'), "rb") as fr:
            self.user_intro = pickle.load(fr)

        # vid
        with open(os.path.join('data/vid', 'vid_'+train_or_test+'.pkl'), "rb") as fr:
            self.vid = pickle.load(fr)

        # c3d
        with open(os.path.join('data/c3d', 'c3d_'+train_or_test+'.pkl'), "rb") as fr:
            self.c3d = pickle.load(fr)

        self.audio = dict(filter(lambda item: item[0] in self.vid, self.audio.items()))
        
        # æ£€æŸ¥æ•°æ®åŠ è½½çŠ¶æ€
        print(f"æ•°æ®é›†åŠ è½½å®Œæˆ. vidé•¿åº¦: {len(self.vid)}, textç±»å‹: {type(self.text)}")
        
        # ç¡®ä¿vidå’ŒtextåŒ¹é…
        if isinstance(self.text, dict):
            # å¦‚æœtextæ˜¯å­—å…¸ï¼Œç¡®ä¿æ‰€æœ‰vidçš„é”®éƒ½å­˜åœ¨
            self.valid_indices = []
            for i, vid_key in enumerate(self.vid):
                if vid_key in self.text and vid_key in self.audio and vid_key in self.comments and vid_key in self.label:
                    self.valid_indices.append(i)
            print(f"æœ‰æ•ˆç´¢å¼•æ•°é‡: {len(self.valid_indices)}/{len(self.vid)}")
        else:
            # å¦‚æœtextæ˜¯åˆ—è¡¨ï¼Œæ£€æŸ¥é•¿åº¦æ˜¯å¦åŒ¹é…
            if len(self.text) != len(self.vid):
                print(f"è­¦å‘Š: texté•¿åº¦({len(self.text)})ä¸vidé•¿åº¦({len(self.vid)})ä¸åŒ¹é…")
                # å–æœ€å°é•¿åº¦
                self.valid_indices = list(range(min(len(self.text), len(self.vid))))
            else:
                self.valid_indices = list(range(len(self.vid)))

    def load_opinion_data(self, opinion_data_path):
        """åŠ è½½éšå¼æ„è§åˆ†ææ•°æ®"""
        try:
            with open(opinion_data_path, 'r', encoding='utf-8') as f:
                opinion_data = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½éšå¼æ„è§æ•°æ®: {len(opinion_data)} æ¡è®°å½•")
            
            # å¦‚æœæ•°æ®æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆä»¥video_idä¸ºé”®ï¼‰
            if isinstance(opinion_data, list):
                video_id_dict = {}
                for data in opinion_data:
                    if isinstance(data, dict) and 'video_id' in data:
                        # ä½¿ç”¨video_idä½œä¸ºé”®ï¼ŒåŒæ—¶æ”¯æŒæ•°å­—å’Œå­—ç¬¦ä¸²æ ¼å¼
                        video_id = data['video_id']
                        video_id_dict[video_id] = data
                        video_id_dict[str(video_id)] = data  # åŒæ—¶å­˜å‚¨å­—ç¬¦ä¸²æ ¼å¼
                print(f"âœ… è½¬æ¢ä¸ºvideo_idå­—å…¸ï¼ŒåŒ…å« {len(video_id_dict)//2} ä¸ªæœ‰æ•ˆæ˜ å°„")
                return video_id_dict
            elif isinstance(opinion_data, dict):
                return opinion_data
            else:
                print(f"âš ï¸ æ„è§æ•°æ®æ ¼å¼ä¸æ”¯æŒï¼Œä½¿ç”¨ç©ºå­—å…¸")
                return {}
                
        except FileNotFoundError:
            print(f"âš ï¸ éšå¼æ„è§æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {opinion_data_path}ï¼Œå°†ä½¿ç”¨ç©ºæ•°æ®")
            return {}
        except Exception as e:
            print(f"âŒ åŠ è½½éšå¼æ„è§æ•°æ®å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç©ºæ•°æ®")
            return {}

    def __len__(self):
        return len(self.valid_indices)
     
    def __getitem__(self, idx):
        # ä½¿ç”¨æœ‰æ•ˆç´¢å¼•
        real_idx = self.valid_indices[idx]
        vid = self.vid[real_idx]
        
        # å‡†å¤‡æ‰€æœ‰æ¨¡æ€æ•°æ®
        if isinstance(self.text, dict):
            text = torch.tensor(self.text[vid], dtype=torch.float32)
        else:
            text = torch.tensor(self.text[real_idx], dtype=torch.float32)
            
        comments = self.comments[vid]
        gpt_description = self.gpt_description[vid]
        audio = torch.tensor(self.audio[vid], dtype=torch.float32)
        video = torch.tensor(self.video[vid], dtype=torch.float32)
        c3d = torch.tensor(self.c3d[vid], dtype=torch.float32)
        label = torch.tensor(self.label[vid])
        user_intro = self.user_intro[vid]

        # è·å–å¯¹åº”çš„éšå¼æ„è§æ•°æ®
        opinion_data = None
        if self.opinion_data:
            # é€šè¿‡video_idåŒ¹é…æ„è§æ•°æ®
            if vid in self.opinion_data:
                opinion_data = self.opinion_data[vid]
                # print(f"ğŸ¯ åŒ¹é…æˆåŠŸ: vid={vid} -> opinion_data")
            elif str(vid) in self.opinion_data:
                opinion_data = self.opinion_data[str(vid)]
                # print(f"ğŸ¯ åŒ¹é…æˆåŠŸ: str(vid)={str(vid)} -> opinion_data")
            # else:
            #     print(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…: vid={vid}, å¯ç”¨é”®ç¤ºä¾‹: {list(self.opinion_data.keys())[:3]}")

        return {
            'label': label,  # æ ‡ç­¾
            'text': text,
            'audioframes': audio,  # éŸ³é¢‘å¸§
            'frames': video,  # å¸§
            'comments': comments, # è¯„è®º
            'c3d': c3d,  # C3Dç‰¹å¾
            'user_intro': user_intro,
            'gpt_description': gpt_description, # gptç”Ÿæˆçš„æ–‡æœ¬è¾…åŠ©åˆ†æ
            'implicit_opinion_data': opinion_data  # éšå¼æ„è§æ•°æ®
        }

def pad_sequence(seq_len,lst, emb):
    result=[]
    for video in lst:
        if isinstance(video, list):
            video = torch.stack(video)
        ori_len=video.shape[0]
        if ori_len == 0:
            video = torch.zeros([seq_len,emb],dtype=torch.long)
        elif ori_len>=seq_len:
            if emb == 200:
                video=torch.FloatTensor(video[:seq_len])
            else:
                video=torch.LongTensor(video[:seq_len])
        else:
            video=torch.cat([video,torch.zeros([seq_len-ori_len,video.shape[1]],dtype=torch.long)],dim=0)
            if emb == 200:
                video=torch.FloatTensor(video)
            else:
                video=torch.LongTensor(video)
        result.append(video)
    return torch.stack(result)

def pad_frame_sequence(seq_len,lst):
    attention_masks = []
    result=[]
    for video in lst:
        # video=torch.FloatTensor(video)
        ori_len=video.shape[0]
        video = video.squeeze()
        if ori_len>=seq_len:
            gap=ori_len//seq_len
            video=video[::gap][:seq_len]
            mask = np.ones((seq_len))
        else:
            video=torch.cat((video, torch.zeros([seq_len-ori_len, video.shape[1]], dtype=torch.float32)), dim=0)
            mask = np.append(np.ones(ori_len), np.zeros(seq_len-ori_len))
        result.append(video)
        mask = torch.IntTensor(mask)
        attention_masks.append(mask)
    return torch.stack(result), torch.stack(attention_masks)

def SVFEND_collate_fn(batch):
    # num_comments = 23
    num_frames = 83
    num_audioframes = 50

    frames = [item['frames'] for item in batch]
    frames, frames_masks = pad_frame_sequence(num_frames, frames)
    frames = frames.squeeze()

    audioframes = [item['audioframes'] for item in batch]
    audioframes, audioframes_masks = pad_frame_sequence(num_audioframes, audioframes)

    comments = [item['comments'] for item in batch]
    # ç¡®ä¿commentsæ˜¯tensor
    processed_comments = []
    for comment in comments:
        # å¦‚æœæ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºtensor
        if isinstance(comment, np.ndarray):
            comment = torch.tensor(comment, dtype=torch.float32)
        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œä¹Ÿè½¬æ¢ä¸ºtensor
        elif isinstance(comment, list):
            comment = torch.tensor(comment, dtype=torch.float32)
        processed_comments.append(comment)
    comments = torch.stack(processed_comments)

    # ç¡®ä¿gpt_descriptionå­—æ®µå­˜åœ¨å¹¶ä¸”æ ¼å¼æ­£ç¡®
    gpt_description = []
    for item in batch:
        if 'gpt_description' in item:
            gpt = item['gpt_description']
        else:
            # å¦‚æœä¸å­˜åœ¨ï¼Œä½¿ç”¨é›¶å‘é‡æ›¿ä»£
            print("è­¦å‘Š: æ ·æœ¬ä¸­ä¸å­˜åœ¨gpt_descriptionå­—æ®µï¼Œä½¿ç”¨é›¶å‘é‡æ›¿ä»£")
            # å‡è®¾ç»´åº¦ä¸æ¨¡å‹ä¸­çš„t_inå‚æ•°ä¸€è‡´ï¼ˆè®ºæ–‡ä¸­ä¸º768ï¼‰
            gpt = np.zeros(768, dtype=np.float32)
        
        # å¤„ç†ä¸åŒçš„æ•°æ®ç±»å‹
        if isinstance(gpt, np.ndarray):
            gpt = torch.tensor(gpt, dtype=torch.float32)
        elif isinstance(gpt, list):
            gpt = torch.tensor(gpt, dtype=torch.float32)
        elif not isinstance(gpt, torch.Tensor):
            # å¦‚æœæ—¢ä¸æ˜¯numpyæ•°ç»„ä¹Ÿä¸æ˜¯åˆ—è¡¨æˆ–tensorï¼Œå°è¯•è½¬æ¢ä¸ºtensor
            try:
                gpt = torch.tensor(gpt, dtype=torch.float32)
            except:
                print(f"è­¦å‘Š: æ— æ³•å°†gpt_descriptionè½¬æ¢ä¸ºtensorï¼Œç±»å‹: {type(gpt)}")
                gpt = torch.zeros(768, dtype=torch.float32)
        
        gpt_description.append(gpt)

    # å°è¯•å †å ï¼Œå¦‚æœå½¢çŠ¶ä¸ä¸€è‡´ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯å¹¶è¿›è¡Œå¤„ç†
    try:
        gpt_description = torch.stack(gpt_description)
    except RuntimeError as e:
        print(f"è­¦å‘Š: åœ¨stack gpt_descriptionæ—¶å‡ºé”™: {e}")
        # æ‰“å°æ¯ä¸ªtensorçš„å½¢çŠ¶ä»¥è¯Šæ–­é—®é¢˜
        for i, gpt in enumerate(gpt_description):
            print(f"  gpt[{i}].shape = {gpt.shape}")
        
        # å°†æ‰€æœ‰tensorè½¬æ¢ä¸ºç›¸åŒçš„å½¢çŠ¶ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªéé›¶tensorçš„å½¢çŠ¶ï¼‰
        target_shape = None
        for gpt in gpt_description:
            if gpt.numel() > 0:
                target_shape = gpt.shape
                break
        
        if target_shape is None:
            target_shape = (768,)  # é»˜è®¤å½¢çŠ¶
        
        processed_gpt = []
        for gpt in gpt_description:
            if gpt.shape != target_shape:
                # å¦‚æœå½¢çŠ¶ä¸åŒ¹é…ï¼Œä½¿ç”¨é›¶tensor
                processed_gpt.append(torch.zeros(target_shape, dtype=torch.float32))
            else:
                processed_gpt.append(gpt)
        
        gpt_description = torch.stack(processed_gpt)

    user_intro = [item['user_intro'] for item in batch]
    # ç¡®ä¿user_introæ˜¯tensor
    processed_user_intro = []
    for intro in user_intro:
        if isinstance(intro, np.ndarray):
            intro = torch.tensor(intro, dtype=torch.float32)
        elif isinstance(intro, list):
            intro = torch.tensor(intro, dtype=torch.float32)
        processed_user_intro.append(intro)
    user_intro = torch.stack(processed_user_intro)

    c3d = [item['c3d'] for item in batch]
    c3d, c3d_masks = pad_frame_sequence(num_frames, c3d)

    label = [item['label'] for item in batch]
    text = [item['text'] for item in batch]
    text = torch.tensor([item.cpu().detach().numpy() for item in text])
    
    # å¤„ç†éšå¼æ„è§æ•°æ®
    implicit_opinion_data = []
    for item in batch:
        if 'implicit_opinion_data' in item and item['implicit_opinion_data'] is not None:
            implicit_opinion_data.append(item['implicit_opinion_data'])
        else:
            implicit_opinion_data.append(None)

    return {
        'label': torch.stack(label),
        'text': text,
        'audioframes': audioframes,
        'audioframes_masks': audioframes_masks,
        'frames': frames,
        'frames_masks': frames_masks,
        'comments': comments,
        'c3d': c3d,
        'c3d_masks': c3d_masks,
        'user_intro': user_intro,
        'gpt_description': gpt_description,
        'implicit_opinion_data': implicit_opinion_data,
    }

def _init_fn(worker_id):
    np.random.seed(2022)

def get_dataloader(modelConfig,data_type='SVFEND'):
    collate_fn=None

    if data_type == 'SVFEND':
        # è·å–éšå¼æ„è§æ•°æ®è·¯å¾„
        opinion_data_path = modelConfig.get("opinion_data_path", "enhanced_results.json")
        
        dataset_train = SVFENDDataset(datamode=modelConfig["datamode"], train_or_test='train', opinion_data_path=opinion_data_path)
        dataset_val = SVFENDDataset(datamode=modelConfig["datamode"], train_or_test='val', opinion_data_path=opinion_data_path)
        dataset_test = SVFENDDataset(datamode=modelConfig["datamode"], train_or_test='test', opinion_data_path=opinion_data_path)
        collate_fn=SVFEND_collate_fn

    # æå–å¯é€‰å‚æ•°
    drop_last = modelConfig.get("drop_last_batch", False)
    num_workers = modelConfig.get("num_workers", 0)
    pin_memory = modelConfig.get("pin_memory", True)
    
    # å¦‚æœå¯ç”¨äº†drop_lastï¼Œæ‰“å°æç¤º
    if drop_last:
        print("å·²å¯ç”¨drop_lastï¼Œå°†ä¸¢å¼ƒä¸å®Œæ•´çš„æœ€åä¸€æ‰¹æ¬¡æ•°æ®")

    train_dataloader = DataLoader(dataset_train, batch_size=modelConfig["batch_size"],
        num_workers=num_workers,
        pin_memory=pin_memory,
        shuffle=True,
        drop_last=drop_last,  # ä½¿ç”¨é…ç½®å‚æ•°
        worker_init_fn=_init_fn,
        collate_fn=collate_fn)
    
    val_dataloader = DataLoader(dataset_val, batch_size=modelConfig["batch_size"],
                                num_workers=num_workers,
                                pin_memory=pin_memory,
                                shuffle=False,
                                drop_last=drop_last,  # å¯¹éªŒè¯é›†ä¹Ÿä½¿ç”¨ç›¸åŒé…ç½®
                                worker_init_fn=_init_fn,
                                collate_fn=collate_fn)
    
    test_dataloader = DataLoader(dataset_test, batch_size=modelConfig["batch_size"],
        num_workers=num_workers,
        pin_memory=pin_memory,
        shuffle=False,
        drop_last=drop_last,  # å¯¹æµ‹è¯•é›†ä¹Ÿä½¿ç”¨ç›¸åŒé…ç½®
        worker_init_fn=_init_fn,
        collate_fn=collate_fn)

    dataloaders = dict(zip(['train', 'val', 'test'], [train_dataloader, val_dataloader, test_dataloader]))

    return dataloaders

def split_word(df):  #å»é™¤åœç”¨è¯
    title = df['description'].values
    comments = df['comments'].apply(lambda x:' '.join(x)).values
    text = np.concatenate([title, comments],axis=0)
    analyse.set_stop_words('./data/stopwords.txt')
    all_word = [analyse.extract_tags(txt) for txt in text.tolist()]
    corpus = [' '.join(word) for word in all_word]
    return corpus