"""
å¤šæ¨¡æ€è™šå‡å†…å®¹æ£€æµ‹ç³»ç»Ÿ - ä¸»è®­ç»ƒè„šæœ¬

âš ï¸ å®‰å…¨è¯´æ˜ï¼š
- æ•æ„Ÿå‚æ•°ï¼ˆæƒé‡ã€é˜ˆå€¼ã€dropoutç­‰ï¼‰ä¸ç¡¬ç¼–ç åœ¨ä»£ç ä¸­
- æ‰€æœ‰æ•æ„Ÿå‚æ•°éœ€é€šè¿‡ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°æä¾›
- è¯¦ç»†é…ç½®è¯´æ˜è¯·å‚è€ƒ CONFIGURATION_GUIDE.md
"""

import os
# è®¾ç½®CUDAå†…å­˜åˆ†é…å™¨å‚æ•°ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

import torch
import torch.nn as nn
import numpy as np
import sys
import random
import torch.backends.cudnn as cudnn
import torchmetrics
import json
from datetime import datetime
import matplotlib.pyplot as plt
import pickle
from pathlib import Path
import time
import argparse

from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

from Diffusion.Multimodal_Diffusion import GaussianDiffusionTrainer
from Diffusion.ExplainableDetection import ExplainableDetection
# from Dataset.dataset import FakeAVCeleb  # æ³¨é‡Šæ‰æ‰¾ä¸åˆ°çš„æ¨¡å—å¯¼å…¥
from modules.MultiGranularityContrast import MultiGranularityContrast
from modules.AdversarialVerification import AdversarialVerification
from modules.NeuralSymbolicRules import NeuralSymbolicRuleEngine
from train import train, valid, calculate_f1, calculate_auc
from dataloader_fakesv import get_dataloader
from eval_metrics import eval_FakeSV

from tensorboardX import SummaryWriter
writer = SummaryWriter("logs")

# åˆ›å»ºä¿å­˜è®­ç»ƒç»“æœçš„ç›®å½•
def ensure_dir(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)

# å°†è®­ç»ƒç»“æœä¿å­˜ä¸ºJSON
def save_training_results(epoch_results, directory="results", filename="training_results.json"):
    """
    å°†è®­ç»ƒç»“æœä¿å­˜åˆ°å•ä¸ªJSONæ–‡ä»¶ä¸­
    
    Args:
        epoch_results: å½“å‰epochçš„ç»“æœå­—å…¸
        directory: ä¿å­˜ç»“æœçš„ç›®å½•
        filename: ä¿å­˜ç»“æœçš„æ–‡ä»¶å
    """
    ensure_dir(directory)
    filepath = f"{directory}/{filename}"
    
    # å°†tensorè½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
    for key, value in epoch_results.items():
        if isinstance(value, torch.Tensor):
            epoch_results[key] = value.item() if value.numel() == 1 else value.tolist()
        elif isinstance(value, dict):
            for k, v in value.items():
                if isinstance(v, torch.Tensor):
                    epoch_results[key][k] = v.item() if v.numel() == 1 else v.tolist()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    all_results = []
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                all_results = json.load(f)
        except json.JSONDecodeError:
            print(f"è­¦å‘Š: {filepath}æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
    
    # æ·»åŠ å½“å‰epochçš„ç»“æœ
    all_results.append(epoch_results)
    
    # ä¿å­˜æ‰€æœ‰ç»“æœ
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=4)
    
    print(f"Epoch {epoch_results['epoch']} ç»“æœå·²æ·»åŠ åˆ° {filepath}")

# ä¿å­˜å¯è§£é‡Šæ€§ç»“æœ
def save_explanation_results(explanations, directory="explanations", filename="explanation_results.pkl"):
    """
    ä¿å­˜å¯è§£é‡Šæ€§ç»“æœ
    
    Args:
        explanations: å¯è§£é‡Šæ€§ç»“æœåˆ—è¡¨
        directory: ä¿å­˜ç›®å½•
        filename: æ–‡ä»¶å
    """
    ensure_dir(directory)
    filepath = os.path.join(directory, filename)
    
    # ä½¿ç”¨pickleä¿å­˜ç»“æœï¼ˆåŒ…å«NumPyæ•°ç»„ï¼‰
    with open(filepath, 'wb') as f:
        pickle.dump(explanations, f)
    
    print(f"å·²ä¿å­˜{len(explanations)}ä¸ªå¯è§£é‡Šæ€§ç»“æœåˆ°{filepath}")
    
    # å¯è§†åŒ–ä¸€éƒ¨åˆ†ç»“æœ
    visualize_explanation_samples(explanations, os.path.join(directory, "visualization"))

# å¯è§†åŒ–éƒ¨åˆ†å¯è§£é‡Šæ€§ç»“æœ
def visualize_explanation_samples(explanations, save_dir, num_samples=5):
    """
    å¯è§†åŒ–éƒ¨åˆ†å¯è§£é‡Šæ€§ç»“æœ
    
    Args:
        explanations: å¯è§£é‡Šæ€§ç»“æœåˆ—è¡¨
        save_dir: ä¿å­˜å¯è§†åŒ–å›¾åƒçš„ç›®å½•
        num_samples: è¦å¯è§†åŒ–çš„æ ·æœ¬æ•°é‡
    """
    ensure_dir(save_dir)
    
    # éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†æ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    if len(explanations) > num_samples:
        indices = np.random.choice(len(explanations), num_samples, replace=False)
        samples = [explanations[i] for i in indices]
    else:
        samples = explanations
    
    for i, sample in enumerate(samples):
        # åˆ›å»ºæ¯ä¸ªæ ·æœ¬çš„å•ç‹¬ç›®å½•
        sample_dir = os.path.join(save_dir, f"sample_{i}")
        ensure_dir(sample_dir)
        
        # è·å–çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
        true_label = sample.get('label', -1)
        pred_label = int(sample.get('predicted_class', -1)) if 'predicted_class' in sample else -1
        
        # å¯è§†åŒ–æ¨¡æ€æƒé‡
        if 'modality_weights' in sample:
            plt.figure(figsize=(8, 5))
            modality_weights = sample['modality_weights']
            modal_names = ["Text", "Audio", "Video"]
            plt.bar(modal_names, modality_weights)
            plt.title(f"Modality Contribution Weights (True: {('Real' if true_label == 0 else 'Fake')}, Pred: {('Real' if pred_label == 0 else 'Fake')})")
            plt.ylim(0, 1)
            plt.savefig(os.path.join(sample_dir, "modality_weights.png"))
            plt.close()
        
        # å¯è§†åŒ–æ–‡æœ¬é‡è¦æ€§
        if 'text_importance' in sample:
            plt.figure(figsize=(10, 2))
            text_importance = sample['text_importance']
            plt.bar(range(len(text_importance)), text_importance)
            plt.title("Text Feature Importance")
            plt.savefig(os.path.join(sample_dir, "text_importance.png"))
            plt.close()
        
        # å¯è§†åŒ–éŸ³é¢‘é‡è¦æ€§
        if 'audio_importance' in sample:
            plt.figure(figsize=(10, 2))
            audio_importance = sample['audio_importance']
            plt.bar(range(len(audio_importance)), audio_importance)
            plt.title("Audio Feature Importance")
            plt.savefig(os.path.join(sample_dir, "audio_importance.png"))
            plt.close()
        
        # å¯è§†åŒ–è§†é¢‘é‡è¦æ€§å’Œçƒ­å›¾
        if 'video_importance' in sample:
            plt.figure(figsize=(10, 2))
            video_importance = sample['video_importance']
            plt.bar(range(len(video_importance)), video_importance)
            plt.title("Video Feature Importance")
            plt.savefig(os.path.join(sample_dir, "video_importance.png"))
            plt.close()
        
        # å¯è§†åŒ–è™šå‡åŒºåŸŸçƒ­å›¾
        if 'fake_region_heatmap' in sample:
            plt.figure(figsize=(8, 3))
            heatmap = sample['fake_region_heatmap']
            plt.imshow(heatmap.reshape(1, -1), cmap='hot', aspect='auto')
            plt.colorbar(label='Fake Level')
            plt.title("Fake Region Heatmap")
            plt.savefig(os.path.join(sample_dir, "fake_region_heatmap.png"))
            
            # ä¿å­˜çƒ­å›¾æ•°æ®
            np.save(os.path.join(sample_dir, "heatmap.npy"), heatmap)
        
        # ä¿å­˜åŸºæœ¬ä¿¡æ¯
        info = {
            'sample_idx': sample.get('sample_idx', i),
            'batch_idx': sample.get('batch_idx', -1),
            'true_label': true_label,
            'predicted_label': pred_label,
            'correct_prediction': true_label == pred_label
        }
        
        with open(os.path.join(sample_dir, "info.json"), 'w') as f:
            json.dump(info, f, indent=4)
    
    print(f"å·²ç”Ÿæˆ{len(samples)}ä¸ªæ ·æœ¬çš„å¯è§†åŒ–ç»“æœï¼Œä¿å­˜åœ¨{save_dir}")

def safe_model_save(trainer, optimizer, valid_acc, epoch, modelConfig, filename):
    """å®‰å…¨åœ°ä¿å­˜æ¨¡å‹ï¼ŒåŒ…å«é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
    save_dir = "model_checkpoints"
    os.makedirs(save_dir, exist_ok=True)
    
    filepath = os.path.join(save_dir, filename)
    temp_filepath = f"{filepath}.tmp"
    
    save_dict = {
        'model_state_dict': trainer.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'valid_acc': valid_acc,
        'epoch': epoch,
        'modelConfig': modelConfig
    }
    
    # å°è¯•ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ŒæˆåŠŸåå†é‡å‘½å
    try:
        torch.save(save_dict, temp_filepath)
        if os.path.exists(filepath):
            os.remove(filepath)
        os.rename(temp_filepath, filepath)
        print(f"æ¨¡å‹æˆåŠŸä¿å­˜åˆ° {filepath}")
        return True
    except Exception as e:
        print(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
        # å°è¯•ä»…ä¿å­˜æ¨¡å‹å‚æ•°
        try:
            print("å°è¯•ä»…ä¿å­˜æ¨¡å‹å‚æ•°...")
            torch.save(trainer.state_dict(), f"{filepath}_params_only.pt")
            print(f"æ¨¡å‹å‚æ•°å·²ä¿å­˜åˆ° {filepath}_params_only.pt")
            return True
        except Exception as e2:
            print(f"ä¿å­˜æ¨¡å‹å‚æ•°ä¹Ÿå¤±è´¥: {e2}")
            return False

def load_sensitive_params_from_env():
    """
    ä»ç¯å¢ƒå˜é‡åŠ è½½æ•æ„Ÿå‚æ•°
    
    æ³¨æ„ï¼šæ•æ„Ÿå‚æ•°ï¼ˆæƒé‡ã€é˜ˆå€¼ç­‰ï¼‰ä¸ç¡¬ç¼–ç ï¼Œéœ€é€šè¿‡ç¯å¢ƒå˜é‡æä¾›
    """
    return {
        # æ¨¡å—æƒé‡å‚æ•°ï¼ˆéœ€é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼‰
        "contrast_weight": float(os.getenv('CONTRAST_WEIGHT', '0.0')),  # éœ€è®¾ç½®
        "adv_weight": float(os.getenv('ADV_WEIGHT', '0.0')),  # éœ€è®¾ç½®
        "neural_symbolic_weight": float(os.getenv('NEURAL_SYMBOLIC_WEIGHT', '0.0')),  # éœ€è®¾ç½®
        "explain_weight": float(os.getenv('EXPLAIN_WEIGHT', '0.0')),  # éœ€è®¾ç½®
        
        # ç¥ç»ç¬¦å·è§„åˆ™å‚æ•°
        "rule_threshold": float(os.getenv('RULE_THRESHOLD', '0.0')),  # éœ€è®¾ç½®
        
        # æ‰©æ•£æ¨¡å‹å‚æ•°
        "beta_1": float(os.getenv('BETA_1', '0.0')),  # éœ€è®¾ç½®
        "beta_T": float(os.getenv('BETA_T', '0.0')),  # éœ€è®¾ç½®
        "diffusion_loss_weight": float(os.getenv('DIFFUSION_LOSS_WEIGHT', '0.0')),  # éœ€è®¾ç½®
        
        # æ­£åˆ™åŒ–å‚æ•°
        "domain_lambda": float(os.getenv('DOMAIN_LAMBDA', '0.0')),  # éœ€è®¾ç½®
        "adv_eps": float(os.getenv('ADV_EPS', '0.0')),  # éœ€è®¾ç½®
        "weight_decay": float(os.getenv('WEIGHT_DECAY', '0.0')),  # éœ€è®¾ç½®
        "label_smoothing": float(os.getenv('LABEL_SMOOTHING', '0.0')),  # éœ€è®¾ç½®
        
        # Dropoutå‚æ•°
        "mult_dropout": float(os.getenv('MULT_DROPOUT', '0.0')),  # éœ€è®¾ç½®
        "Text_Pre_dropout": float(os.getenv('TEXT_PRE_DROPOUT', '0.0')),  # éœ€è®¾ç½®
        "Img_Pre_dropout": float(os.getenv('IMG_PRE_DROPOUT', '0.0')),  # éœ€è®¾ç½®
        "comments_dropout": float(os.getenv('COMMENTS_DROPOUT', '0.0')),  # éœ€è®¾ç½®
        
        # å¯¹æ¯”å­¦ä¹ å‚æ•°
        "contrast_temperature": float(os.getenv('CONTRAST_TEMPERATURE', '0.0')),  # éœ€è®¾ç½®
        "contrast_projection_dim": int(os.getenv('CONTRAST_PROJECTION_DIM', '0')),  # éœ€è®¾ç½®
        
        # å¯¹æŠ—éªŒè¯å‚æ•°
        "adv_dropout": float(os.getenv('ADV_DROPOUT', '0.0')),  # éœ€è®¾ç½®
        "adv_hidden_dim": int(os.getenv('ADV_HIDDEN_DIM', '0')),  # éœ€è®¾ç½®
    }

def parse_arguments():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    æ³¨æ„ï¼šæ•æ„Ÿå‚æ•°ï¼ˆæƒé‡ã€é˜ˆå€¼ï¼‰ä¸åœ¨æ­¤å¤„ç¡¬ç¼–ç ï¼Œéœ€é€šè¿‡ç¯å¢ƒå˜é‡æä¾›
    """
    parser = argparse.ArgumentParser(description='å¤šæ¨¡æ€è™šå‡è§†é¢‘æ£€æµ‹ç³»ç»Ÿ - æ¨¡å—åŒ–è¿è¡Œ')
    
    # åŸºç¡€è®­ç»ƒå‚æ•°
    parser.add_argument('--epoch', type=int, default=60, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=7e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--device', type=str, default='auto', help='è®¾å¤‡é€‰æ‹© (auto/cuda:0/cpu)')
    
    # åŠŸèƒ½æ¨¡å—å¼€å…³
    parser.add_argument('--use_explain', type=str, default='True', 
                       help='æ˜¯å¦å¯ç”¨å¯è§£é‡Šæ€§æ¨¡å— (True/False)')
    parser.add_argument('--use_multi_granularity_contrast', type=str, default='True',
                       help='æ˜¯å¦å¯ç”¨å¤šç²’åº¦å¯¹æ¯”å­¦ä¹  (True/False)')
    parser.add_argument('--use_adversarial_verification', type=str, default='True',
                       help='æ˜¯å¦å¯ç”¨å¯¹æŠ—æ€§éªŒè¯æ¡†æ¶ (True/False)')
    parser.add_argument('--use_neural_symbolic', type=str, default='True',
                       help='æ˜¯å¦å¯ç”¨ç¥ç»ç¬¦å·è§„åˆ™ç³»ç»Ÿ (True/False)')
    
    # æ¨¡å—æƒé‡å‚æ•°ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œä¸ç¡¬ç¼–ç é»˜è®¤å€¼ï¼‰
    parser.add_argument('--contrast_weight', type=float, default=None, 
                       help='å¯¹æ¯”å­¦ä¹ æŸå¤±æƒé‡ï¼ˆéœ€é€šè¿‡ç¯å¢ƒå˜é‡CONTRAST_WEIGHTè®¾ç½®ï¼‰')
    parser.add_argument('--adv_weight', type=float, default=None, 
                       help='å¯¹æŠ—éªŒè¯æŸå¤±æƒé‡ï¼ˆéœ€é€šè¿‡ç¯å¢ƒå˜é‡ADV_WEIGHTè®¾ç½®ï¼‰')
    parser.add_argument('--neural_symbolic_weight', type=float, default=None, 
                       help='ç¥ç»ç¬¦å·è§„åˆ™æƒé‡ï¼ˆéœ€é€šè¿‡ç¯å¢ƒå˜é‡NEURAL_SYMBOLIC_WEIGHTè®¾ç½®ï¼‰')
    parser.add_argument('--explain_weight', type=float, default=None, 
                       help='å¯è§£é‡Šæ€§æŸå¤±æƒé‡ï¼ˆéœ€é€šè¿‡ç¯å¢ƒå˜é‡EXPLAIN_WEIGHTè®¾ç½®ï¼‰')
    
    # ç¥ç»ç¬¦å·è§„åˆ™ç‰¹å®šå‚æ•°
    parser.add_argument('--rule_threshold', type=float, default=None, 
                       help='è§„åˆ™æ¿€æ´»é˜ˆå€¼ï¼ˆéœ€é€šè¿‡ç¯å¢ƒå˜é‡RULE_THRESHOLDè®¾ç½®ï¼‰')
    parser.add_argument('--enable_implicit_analysis', type=str, default='False',
                       help='æ˜¯å¦å¯ç”¨å®æ—¶éšå¼æ„è§åˆ†æ (True/False)')
    parser.add_argument('--opinion_data_path', type=str, default='enhanced_results.json',
                       help='éšå¼æ„è§åˆ†ææ•°æ®æ–‡ä»¶è·¯å¾„')
    
    # è°ƒè¯•å’Œæ—¥å¿—å‚æ•°
    parser.add_argument('--debug_neural_symbolic', type=str, default='True',
                       help='æ˜¯å¦å¯ç”¨ç¥ç»ç¬¦å·è§„åˆ™è°ƒè¯•è¾“å‡º (True/False)')
    parser.add_argument('--save_dir', type=str, default='model_checkpoints',
                       help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--log_interval', type=int, default=10, help='æ—¥å¿—è¾“å‡ºé—´éš”')
    
    # æ•°æ®ç›¸å…³å‚æ•°
    parser.add_argument('--dataset', type=str, default='SVFEND', help='æ•°æ®é›†åç§°')
    parser.add_argument('--datamode', type=str, default='title+ocr', help='æ•°æ®æ¨¡å¼')
    
    return parser.parse_args()

def str_to_bool(v):
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¸ƒå°”å€¼"""
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

def create_model_config(args):
    """
    æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºæ¨¡å‹é…ç½®
    
    æ³¨æ„ï¼šæ•æ„Ÿå‚æ•°ï¼ˆæƒé‡ã€é˜ˆå€¼ã€dropoutç­‰ï¼‰ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œä¸ç¡¬ç¼–ç 
    """
    # ä»ç¯å¢ƒå˜é‡åŠ è½½æ•æ„Ÿå‚æ•°
    sensitive_params = load_sensitive_params_from_env()
    
    # è®¾å¤‡é…ç½®
    if args.device == 'auto':
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device
    
    # æƒé‡å‚æ•°ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ï¼Œæœ€åæŠ¥é”™
    contrast_weight = args.contrast_weight if args.contrast_weight is not None else sensitive_params["contrast_weight"]
    adv_weight = args.adv_weight if args.adv_weight is not None else sensitive_params["adv_weight"]
    neural_symbolic_weight = args.neural_symbolic_weight if args.neural_symbolic_weight is not None else sensitive_params["neural_symbolic_weight"]
    explain_weight = args.explain_weight if args.explain_weight is not None else sensitive_params["explain_weight"]
    rule_threshold = args.rule_threshold if args.rule_threshold is not None else sensitive_params["rule_threshold"]
    
    # éªŒè¯æ•æ„Ÿå‚æ•°æ˜¯å¦å·²è®¾ç½®
    if contrast_weight == 0.0 and str_to_bool(args.use_multi_granularity_contrast):
        print("âš ï¸ è­¦å‘Š: CONTRAST_WEIGHTæœªè®¾ç½®ï¼Œå¯¹æ¯”å­¦ä¹ æ¨¡å—å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
    if adv_weight == 0.0 and str_to_bool(args.use_adversarial_verification):
        print("âš ï¸ è­¦å‘Š: ADV_WEIGHTæœªè®¾ç½®ï¼Œå¯¹æŠ—éªŒè¯æ¨¡å—å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
    if neural_symbolic_weight == 0.0 and str_to_bool(args.use_neural_symbolic):
        print("âš ï¸ è­¦å‘Š: NEURAL_SYMBOLIC_WEIGHTæœªè®¾ç½®ï¼Œç¥ç»ç¬¦å·è§„åˆ™æ¨¡å—å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
    if rule_threshold == 0.0 and str_to_bool(args.use_neural_symbolic):
        print("âš ï¸ è­¦å‘Š: RULE_THRESHOLDæœªè®¾ç½®ï¼Œç¥ç»ç¬¦å·è§„åˆ™æ¨¡å—å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
    
    modelConfig = {
        "state": "train",
        "epoch": args.epoch,
        "batch_size": args.batch_size,
        "T": 100,  # æ‰©æ•£æ­¥æ•°ï¼ˆéæ•æ„Ÿå‚æ•°ï¼‰
        # Dropoutå‚æ•°ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        "mult_dropout": sensitive_params["mult_dropout"] if sensitive_params["mult_dropout"] > 0 else 0.4,
        "Text_Pre_dropout": sensitive_params["Text_Pre_dropout"] if sensitive_params["Text_Pre_dropout"] > 0 else 0.3,
        "Img_Pre_dropout": sensitive_params["Img_Pre_dropout"] if sensitive_params["Img_Pre_dropout"] > 0 else 0.3,
        "comments_dropout": sensitive_params["comments_dropout"] if sensitive_params["comments_dropout"] > 0 else 0.3,
        "lr": args.lr,
        # æ‰©æ•£æ¨¡å‹å‚æ•°ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        "beta_1": sensitive_params["beta_1"] if sensitive_params["beta_1"] > 0 else 1e-4,
        "beta_T": sensitive_params["beta_T"] if sensitive_params["beta_T"] > 0 else 0.02,
        "device": device,
        # ç‰¹å¾ç»´åº¦ï¼ˆéæ•æ„Ÿå‚æ•°ï¼‰
        "t_in": 768,
        "i_in": 2048,
        "a_in": 128,
        "v_in": 4096,
        "c3d_in": 4096,
        "t_in_pre": 100,
        "a_in_pre": 128,
        "v_in_pre": 1000,
        "c3d_in_pre": 128,
        "label_dim": 2,
        "d_m": 128,
        "unified_size": 128,
        "vertex_num": 32,
        "routing": 2,
        "T_t": 2,
        "T_a": 2,
        "T_v": 2,
        # æ­£åˆ™åŒ–å‚æ•°ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        "weight_decay": sensitive_params["weight_decay"] if sensitive_params["weight_decay"] > 0 else 0.05,
        "num_workers": 4,
        "save_freq": 5,
        "early_stop": 15,
        "use_lr_scheduler": False,
        "lr_scheduler_patience": 3,
        "lr_scheduler_factor": 0.5,
        
        # åŠŸèƒ½æ¨¡å—é…ç½® - ä»å‘½ä»¤è¡Œå‚æ•°è¯»å–
        "use_explain": str_to_bool(args.use_explain),
        "use_multi_granularity_contrast": str_to_bool(args.use_multi_granularity_contrast),
        "use_adversarial_verification": str_to_bool(args.use_adversarial_verification),
        "use_neural_symbolic": str_to_bool(args.use_neural_symbolic),
        
        # æƒé‡é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°è¯»å–ï¼‰
        "contrast_weight": contrast_weight,
        "adv_weight": adv_weight,
        "neural_symbolic_weight": neural_symbolic_weight,
        "explain_weight": explain_weight,
        "domain_lambda": sensitive_params["domain_lambda"] if sensitive_params["domain_lambda"] > 0 else 0.05,
        "adv_eps": sensitive_params["adv_eps"] if sensitive_params["adv_eps"] > 0 else 0.05,
        
        # ç¥ç»ç¬¦å·è§„åˆ™é…ç½®
        "enable_neural_symbolic": str_to_bool(args.use_neural_symbolic),
        "rule_threshold": rule_threshold,
        "enable_implicit_analysis": str_to_bool(args.enable_implicit_analysis),
        "opinion_data_path": args.opinion_data_path,
        "debug_neural_symbolic": str_to_bool(args.debug_neural_symbolic),
        "log_rule_applications": True,
        "save_rule_statistics": True,
        
        # æ•°æ®åŠ è½½ä¸å¤„ç†å‚æ•°
        "datamode": args.datamode,
        "dataset": args.dataset,
        "drop_last_batch": False,  
        "skip_error_batches": True,
        "pin_memory": True,
        
        # å­¦ä¹ ç‡è°ƒåº¦å‚æ•°
        "use_lr_scheduler": True,
        "lr_patience": 5,
        "lr_factor": 0.7,
        "lr_threshold": 1e-4,
        "lr_min": 1e-7,
        
        # æ¢¯åº¦è£å‰ªä¸ä¼˜åŒ–å™¨å‚æ•°
        "clip_grad_norm": 1.0,
        "weight_init": "xavier_normal",
        "betas": (0.9, 0.999),
        "eps": 1e-8,
        
        # æ­£åˆ™åŒ–ä¸è®­ç»ƒç¨³å®šæ€§å‚æ•°ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        "label_smoothing": sensitive_params["label_smoothing"] if sensitive_params["label_smoothing"] > 0 else 0.1,
        "use_amp": True,
        "warmup_steps": 1000,
        "use_warmup": True,
        
        # æ‰©æ•£æ¨¡å‹ç‰¹å®šå‚æ•°ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        "diffusion_loss_weight": sensitive_params["diffusion_loss_weight"] if sensitive_params["diffusion_loss_weight"] > 0 else 0.008,
        
        # å¯¹æ¯”å­¦ä¹ å‚æ•°ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        "contrast_temperature": sensitive_params["contrast_temperature"] if sensitive_params["contrast_temperature"] > 0 else 0.1,
        "contrast_projection_dim": sensitive_params["contrast_projection_dim"] if sensitive_params["contrast_projection_dim"] > 0 else 64,
        "contrast_spatial_regions": 4,
        "contrast_temporal_segments": 8,
        
        # å¯¹æŠ—éªŒè¯å‚æ•°ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        "adv_dropout": sensitive_params["adv_dropout"] if sensitive_params["adv_dropout"] > 0 else 0.3,
        "adv_hidden_dim": sensitive_params["adv_hidden_dim"] if sensitive_params["adv_hidden_dim"] > 0 else 256,
        "adv_z_dim": 64,
        "adv_num_layers": 2,
        
        # ä¿å­˜ç›®å½•é…ç½®
        "save_dir": args.save_dir,
        "log_interval": args.log_interval,
    }
    
    return modelConfig

def print_module_status(config):
    """
    æ‰“å°å½“å‰å¯ç”¨çš„æ¨¡å—çŠ¶æ€
    """
    print("="*60)
    print("ğŸ”§ æ¨¡å—é…ç½®çŠ¶æ€")
    print("="*60)
    print(f"ğŸ§  å¯è§£é‡Šæ€§æ¨¡å—: {'âœ… å¯ç”¨' if config['use_explain'] else 'âŒ å…³é—­'}")
    print(f"ğŸ” å¤šç²’åº¦å¯¹æ¯”å­¦ä¹ : {'âœ… å¯ç”¨' if config['use_multi_granularity_contrast'] else 'âŒ å…³é—­'}")
    print(f"ğŸ›¡ï¸ å¯¹æŠ—æ€§éªŒè¯æ¡†æ¶: {'âœ… å¯ç”¨' if config['use_adversarial_verification'] else 'âŒ å…³é—­'}")
    print(f"âš–ï¸ ç¥ç»ç¬¦å·è§„åˆ™: {'âœ… å¯ç”¨' if config['use_neural_symbolic'] else 'âŒ å…³é—­'}")
    print()
    
    if config['use_neural_symbolic']:
        print("ğŸ¯ ç¥ç»ç¬¦å·è§„åˆ™è¯¦ç»†é…ç½®:")
        print(f"   - è§„åˆ™æƒé‡: {config['neural_symbolic_weight']}")
        print(f"   - æ¿€æ´»é˜ˆå€¼: {config['rule_threshold']}")
        print(f"   - å®æ—¶åˆ†æ: {'å¯ç”¨' if config['enable_implicit_analysis'] else 'å…³é—­'}")
        print(f"   - æ•°æ®è·¯å¾„: {config['opinion_data_path']}")
        print(f"   - è°ƒè¯•è¾“å‡º: {'å¯ç”¨' if config['debug_neural_symbolic'] else 'å…³é—­'}")
        print()
    
    print("âš™ï¸ æƒé‡é…ç½®:")
    if config['use_explain']:
        print(f"   - å¯è§£é‡Šæ€§æƒé‡: {config['explain_weight']}")
    if config['use_multi_granularity_contrast']:
        print(f"   - å¯¹æ¯”å­¦ä¹ æƒé‡: {config['contrast_weight']}")
    if config['use_adversarial_verification']:
        print(f"   - å¯¹æŠ—éªŒè¯æƒé‡: {config['adv_weight']}")
    if config['use_neural_symbolic']:
        print(f"   - ç¥ç»ç¬¦å·æƒé‡: {config['neural_symbolic_weight']}")
    print("="*60)

def main(external_config=None):
    """
    ä¸»å‡½æ•°ï¼šè§£æå‚æ•°å¹¶å¯åŠ¨è®­ç»ƒ
    
    Args:
        external_config: å¤–éƒ¨ä¼ å…¥çš„é…ç½®ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨è¯¥é…ç½®è€Œä¸æ˜¯è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    if external_config is not None:
        # ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„é…ç½®
        modelConfig = external_config
        print("âœ… ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„é…ç½®")
    else:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parse_arguments()
        
        # åˆ›å»ºæ¨¡å‹é…ç½®
        modelConfig = create_model_config(args)
        
        # æ‰“å°æ¨¡å—çŠ¶æ€
        print_module_status(modelConfig)
    
    # å¦‚æœç”¨æˆ·é€‰æ‹©äº†æ¨¡å—ï¼Œç¡®ä¿ç›¸å…³ä¾èµ–å¯ç”¨
    if modelConfig['use_neural_symbolic']:
        try:
            from modules.NeuralSymbolicRules import NeuralSymbolicRuleEngine
            print("âœ… ç¥ç»ç¬¦å·è§„åˆ™æ¨¡å—å¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âŒ æ— æ³•å¯¼å…¥ç¥ç»ç¬¦å·è§„åˆ™æ¨¡å—: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…ç›¸å…³ä¾èµ–")
            return -1
    
    # å¼€å§‹è®­ç»ƒæµç¨‹
    print("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹...")
    
    # æ•°æ®åŠ è½½
    device = torch.device(modelConfig["device"])
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    if device.type == 'cuda':
        print(f"   GPUåç§°: {torch.cuda.get_device_name(device)}")
        print(f"   å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"   å½“å‰GPUå†…å­˜: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")

    print("ğŸ“Š å¼€å§‹åŠ è½½æ•°æ®...")
    dataloader = get_dataloader(modelConfig=modelConfig, data_type='SVFEND')
    print("âœ… æ•°æ®åŠ è½½å®Œæˆ")

    # æ¨¡å‹åˆå§‹åŒ–
    print("ğŸ”§ åˆå§‹åŒ–æ¨¡å‹...")
    trainer = GaussianDiffusionTrainer(
        modelConfig, modelConfig["beta_1"], modelConfig["beta_T"], modelConfig["T"],
        modelConfig["t_in"], modelConfig["a_in"], modelConfig["v_in"], modelConfig["d_m"], modelConfig["mult_dropout"],
        modelConfig["label_dim"],
        modelConfig["unified_size"], modelConfig["vertex_num"], modelConfig["routing"], modelConfig["T_t"],
        modelConfig["T_a"],  modelConfig["T_v"], modelConfig["batch_size"]).to(device)
    print("âœ… ä¸»æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    # åº”ç”¨æƒé‡åˆå§‹åŒ–
    if modelConfig["weight_init"] == "xavier_normal":
        print("åº”ç”¨Xavier Normalæƒé‡åˆå§‹åŒ–...")
        for p in trainer.parameters():
            if p.dim() > 1:
                nn.init.xavier_normal_(p)
    elif modelConfig["weight_init"] == "kaiming_normal":
        print("åº”ç”¨Kaiming Normalæƒé‡åˆå§‹åŒ–...")
        for p in trainer.parameters():
            if p.dim() > 1:
                nn.init.kaiming_normal_(p, nonlinearity='relu')
    
    # åˆå§‹åŒ–å¤šç²’åº¦å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    contrast_module = None
    if modelConfig.get("use_multi_granularity_contrast", False):
        print("åˆå§‹åŒ–å¤šç²’åº¦å¯¹æ¯”å­¦ä¹ æ¨¡å—...")
        # å‚æ•°ä»é…ç½®è¯»å–ï¼ˆå·²ä»ç¯å¢ƒå˜é‡åŠ è½½ï¼‰
        contrast_module = MultiGranularityContrast(
            feature_dim=modelConfig["unified_size"],  # ä½¿ç”¨ç»Ÿä¸€ç‰¹å¾ç»´åº¦
            projection_dim=modelConfig.get("contrast_projection_dim", 64),
            temperature=modelConfig.get("contrast_temperature", 0.1),
            spatial_regions=modelConfig.get("contrast_spatial_regions", 4),
            temporal_segments=modelConfig.get("contrast_temporal_segments", 8),
            modal_components=3  # æ–‡æœ¬ã€éŸ³é¢‘ã€è§†é¢‘ä¸‰ç§æ¨¡æ€
        ).to(device)
        print(f"å¤šç²’åº¦å¯¹æ¯”å­¦ä¹ é…ç½®: ç‰¹å¾ç»´åº¦={modelConfig['unified_size']}, "
              f"æ¸©åº¦={modelConfig.get('contrast_temperature', 0.1)} "
              f"(ä»ç¯å¢ƒå˜é‡CONTRAST_TEMPERATUREè¯»å–)")
    
    # åˆå§‹åŒ–å¯¹æŠ—æ€§éªŒè¯æ¡†æ¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    adv_framework = None
    if modelConfig.get("use_adversarial_verification", False):
        print("åˆå§‹åŒ–å¯¹æŠ—æ€§éªŒè¯æ¡†æ¶...")
        from modules.AdversarialVerification import AdversarialVerification
        # å‚æ•°ä»é…ç½®è¯»å–ï¼ˆå·²ä»ç¯å¢ƒå˜é‡åŠ è½½ï¼‰
        adv_framework = AdversarialVerification(
            feature_dim=modelConfig["unified_size"],
            hidden_dim=modelConfig.get("adv_hidden_dim", 256),
            z_dim=modelConfig.get("adv_z_dim", 64),
            num_layers=modelConfig.get("adv_num_layers", 2),
            dropout=modelConfig.get("adv_dropout", 0.3)
        ).to(device)
        print(f"å¯¹æŠ—æ€§éªŒè¯æ¡†æ¶é…ç½®: ç‰¹å¾ç»´åº¦={modelConfig['unified_size']}, "
              f"éšè—ç»´åº¦={modelConfig.get('adv_hidden_dim', 256)} "
              f"(ä»ç¯å¢ƒå˜é‡ADV_HIDDEN_DIMè¯»å–), "
              f"Dropout={modelConfig.get('adv_dropout', 0.3)} "
              f"(ä»ç¯å¢ƒå˜é‡ADV_DROPOUTè¯»å–)")

    optimizer = torch.optim.AdamW(
        trainer.parameters(), 
        lr=modelConfig["lr"], 
        weight_decay=modelConfig["weight_decay"],
        betas=modelConfig.get("betas", (0.9, 0.999)),
        eps=modelConfig.get("eps", 1e-8)
    )
    
    # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
    if modelConfig.get("use_lr_scheduler", False):
        from torch.optim.lr_scheduler import ReduceLROnPlateau
        scheduler = ReduceLROnPlateau(
            optimizer, 
            mode='max',  # ç›‘æ§éªŒè¯å‡†ç¡®ç‡
            factor=modelConfig.get("lr_factor", 0.7),
            patience=modelConfig.get("lr_patience", 5),
            verbose=True,
            threshold=modelConfig.get("lr_threshold", 1e-4),
            min_lr=modelConfig.get("lr_min", 1e-7)
        )
        print(f"å·²å¯ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ - å‚æ•°: patience={modelConfig.get('lr_patience', 5)}, factor={modelConfig.get('lr_factor', 0.7)}")
    else:
        scheduler = None
    
    # æ·»åŠ Warmupè°ƒåº¦å™¨
    if modelConfig.get("use_warmup", False):
        print(f"å¯ç”¨å­¦ä¹ ç‡é¢„çƒ­ï¼ˆWarmupï¼‰- é¢„çƒ­æ­¥æ•°: {modelConfig.get('warmup_steps', 1000)}")
        # è¿™é‡Œåªæ˜¯è®°å½•warmupçŠ¶æ€ï¼Œå®é™…å®ç°åœ¨è®­ç»ƒå¾ªç¯ä¸­
        
    # æŸå¤±å‡½æ•° - æ·»åŠ æ ‡ç­¾å¹³æ»‘
    if modelConfig.get("label_smoothing", 0) > 0:
        print(f"ä½¿ç”¨æ ‡ç­¾å¹³æ»‘ï¼Œå¹³æ»‘ç³»æ•°: {modelConfig['label_smoothing']}")
        criterion = nn.CrossEntropyLoss(label_smoothing=modelConfig["label_smoothing"]).to(device)
    else:
        criterion = nn.CrossEntropyLoss().to(device)

    if modelConfig["dataset"] in ['WEIBO']:
        best_valid_acc = -1
        epoch, best_epoch = 0, 0
        global_step = 0  # å…¨å±€æ­¥æ•°è®¡æ•°å™¨ï¼Œç”¨äºwarmup
    else:
        # å¯¹äºå…¶ä»–æ•°æ®é›†ï¼Œä¹Ÿåˆå§‹åŒ–ç›¸å…³å˜é‡
        best_valid_acc = -1  # åˆå§‹åŒ–ä¸ºä¸€ä¸ªè¾ƒå°çš„å€¼
        epoch, best_epoch = 0, 0
        global_step = 0

    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•ä¸æ–‡ä»¶å
    results_dir = f"results_{modelConfig['dataset']}"
    results_filename = f"{modelConfig['dataset']}_training_results.json"
    ensure_dir(results_dir)
    
    # åˆ›å»ºå¯è§£é‡Šæ€§ç»“æœä¿å­˜ç›®å½•
    explanation_dir = modelConfig.get("explanation_dir", "explanations")
    ensure_dir(explanation_dir)
    
    # æ¯ä¸ªepochä¿å­˜æœ€ä½³çš„å¯è§£é‡Šæ€§ç»“æœ
    best_explanation_results = None

    # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler() if modelConfig.get("use_amp", False) and torch.cuda.is_available() else None
    if scaler:
        print("å·²å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
        
    # è¾“å‡ºæ˜¯å¦å¯ç”¨å¯è§£é‡Šæ€§
    if modelConfig.get("enable_explanation", False):
        print(f"å·²å¯ç”¨å¯è§£é‡Šæ€§æ¨¡å— - å¯è§†åŒ–ç»“æœå°†ä¿å­˜åœ¨ {explanation_dir} ç›®å½•ä¸­")

    # è®¾ç½®éšæœºç§å­
    setup_seed(42)
    
    # ä¿®æ”¹è®­ç»ƒå‡½æ•°ä»¥æ”¯æŒå¤šç²’åº¦å¯¹æ¯”å­¦ä¹ å’Œå¯¹æŠ—æ€§éªŒè¯æ¡†æ¶
    from train import train
    
    # ä¿®æ”¹ä¸ºæ”¯æŒæ–°æ¨¡å—çš„è®­ç»ƒå‡½æ•°è°ƒç”¨
    best_val_acc = -1
    best_model_path = ''
    patience_counter = 0
    
    for epoch in range(modelConfig["epoch"]):
        # è®­ç»ƒä¸€ä¸ªepoch
        train_loss, train_acc, valid_loss, valid_acc, explanations = train(
            trainer, device, dataloader["train"], dataloader["val"], optimizer, epoch,
            modelConfig, criterion=criterion, contrast_module=contrast_module, adv_framework=adv_framework
        )
        
        # è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½
        print("è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½...")
        trainer.eval()
        with torch.no_grad():
            test_loss, test_results, test_truths, _ = valid(dataloader["test"], trainer, criterion, modelConfig)
            test_acc = 0.0
            if len(test_results) > 0 and len(test_truths) > 0:
                test_acc = (test_results == test_truths).float().mean().item()
            
            # è®¡ç®—å…¶ä»–æµ‹è¯•æŒ‡æ ‡
            test_predictions = test_results.cpu().numpy()
            test_labels = test_truths.cpu().numpy()
            try:
                test_f1 = calculate_f1(test_predictions, test_labels)
                test_auc = calculate_auc(test_predictions, test_labels)
            except:
                test_f1 = 0.0
                test_auc = 0.0
                
            print(f"æµ‹è¯•é›†è¯„ä¼°: Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}, AUC: {test_auc:.4f}")
        
        # ä¿å­˜ç»“æœåˆ°epoch_resultsåˆ—è¡¨
        epoch_results = {
            'epoch': epoch,
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'train': {
                'loss': train_loss,
                'accuracy': train_acc,
            },
            'validation': {
                'loss': valid_loss,
                'accuracy': valid_acc,
            },
            'test': {  # æ·»åŠ æµ‹è¯•é›†çš„ç»“æœ
                'loss': test_loss,
                'accuracy': test_acc,
                'f1_score': test_f1,
                'auc': test_auc
            },
            'hyperparameters': {
                'learning_rate': optimizer.param_groups[0]['lr'],
                'batch_size': modelConfig['batch_size'],
                'dropout': modelConfig['mult_dropout'],
                'weight_decay': modelConfig['weight_decay'],
                'diffusion_loss_weight': modelConfig.get('diffusion_loss_weight', 0.008),
                'contrast_weight': modelConfig.get('contrast_weight', 0.1) if modelConfig.get('use_multi_granularity_contrast', False) else 0,
                'adv_weight': modelConfig.get('adv_weight', 0.1) if modelConfig.get('use_adversarial_verification', False) else 0
            },
            'best_so_far': valid_acc >= best_val_acc
        }
        
        # ä¿å­˜è®­ç»ƒç»“æœåˆ°å•ä¸ªJSONæ–‡ä»¶
        save_training_results(epoch_results, results_dir, results_filename)
        
        # ä¿å­˜å¯è§£é‡Šæ€§ç»“æœ
        if modelConfig.get("enable_explanation", False) and modelConfig.get("save_explanations", False):
            # ä½¿ç”¨æµ‹è¯•é›†çš„å¯è§£é‡Šæ€§ç»“æœ
            epoch_explanation_dir = os.path.join(explanation_dir, f"epoch_{epoch}")
            ensure_dir(epoch_explanation_dir)
            
            # ä¿å­˜æœ¬è½®çš„å¯è§£é‡Šæ€§ç»“æœ
            explanation_filename = f"{modelConfig['dataset']}_explanations_epoch_{epoch}.pkl"
            save_explanation_results(explanations, epoch_explanation_dir, explanation_filename)
            
            # å¯è§†åŒ–éƒ¨åˆ†ç»“æœ
            num_vis_samples = min(len(explanations), modelConfig.get("visualization_samples", 10))
            visualize_explanation_samples(
                explanations, 
                os.path.join(epoch_explanation_dir, "visualization"),
                num_samples=num_vis_samples
            )
            
            # å¦‚æœæ˜¯æœ€ä½³epochï¼Œä¿å­˜ä¸ºæœ€ä½³å¯è§£é‡Šæ€§ç»“æœ
            if valid_acc >= best_valid_acc:
                best_explanation_results = explanations
        
        # è°ƒç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œæ ¹æ®éªŒè¯å‡†ç¡®ç‡è°ƒæ•´å­¦ä¹ ç‡
        if scheduler is not None and modelConfig.get("use_lr_scheduler", False):
            scheduler.step(valid_acc)
            current_lr = optimizer.param_groups[0]['lr']
            print(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
        
        # æ‰“å°è¿›åº¦
        print(f"Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, " +
              f"Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}")
        
        # ä¿å­˜æœ€å¥½çš„æ¨¡å‹
        if valid_acc > best_val_acc:
            best_val_acc = valid_acc
            patience_counter = 0
            
            # ä¿å­˜æ¨¡å‹
            best_model_path = os.path.join(results_dir, f"best_model.pth")
            torch.save({
                'epoch': epoch,
                'model_state_dict': trainer.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'valid_acc': valid_acc,
            }, best_model_path)
            print(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {valid_acc:.4f}")
            
            # å¯è§†åŒ–æœ€ä½³æ¨¡å‹çš„è§£é‡Šç»“æœ
            if modelConfig.get("use_explain", True) and explanations:
                visualize_explanation_samples(explanations, os.path.join(explanation_dir, f"epoch_{epoch}"))
                print(f"ä¿å­˜äº†è§£é‡Šç»“æœåˆ° {explanation_dir}/epoch_{epoch}")
        else:
            patience_counter += 1
            print(f"éªŒè¯å‡†ç¡®ç‡æœªæé«˜, è€å¿ƒè®¡æ•°: {patience_counter}/{modelConfig['early_stop']}")
        
        # æ¯save_freqä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
        if (epoch + 1) % modelConfig["save_freq"] == 0:
            checkpoint_path = os.path.join(results_dir, f"checkpoint_epoch_{epoch}.pth")
            torch.save({
                'epoch': epoch,
                'model_state_dict': trainer.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'valid_acc': valid_acc,
            }, checkpoint_path)
            print(f"ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {checkpoint_path}")
        
        # æ—©åœ
        if patience_counter >= modelConfig["early_stop"]:
            print(f"è¿ç»­ {modelConfig['early_stop']} ä¸ªepochéªŒè¯å‡†ç¡®ç‡æœªæé«˜ï¼Œæ—©åœ")
            break
        
        writer.close()
    
    # è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°
    print("è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°...")
    if os.path.exists(best_model_path):
        # åŠ è½½æœ€ä½³æ¨¡å‹
        checkpoint = torch.load(best_model_path)
        trainer.load_state_dict(checkpoint['model_state_dict'])
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        trainer.eval()
        with torch.no_grad():
            test_loss, test_results, test_truths, _ = valid(dataloader["test"], trainer, criterion, modelConfig)
            test_acc = 0.0
            if len(test_results) > 0 and len(test_truths) > 0:
                test_acc = (test_results == test_truths).float().mean().item()
            
            # è®¡ç®—å…¶ä»–è¯„ä¼°æŒ‡æ ‡
            test_predictions = test_results.cpu().numpy()
            test_labels = test_truths.cpu().numpy()
            try:
                test_f1 = calculate_f1(test_predictions, test_labels)
                test_auc = calculate_auc(test_predictions, test_labels)
            except:
                test_f1 = 0.0
                test_auc = 0.0
            
            # ä¿å­˜æœ€ç»ˆæµ‹è¯•ç»“æœ
            final_test_results = {
                'final_test': {
                    'loss': test_loss,
                    'accuracy': test_acc,
                    'f1_score': test_f1,
                    'auc': test_auc
                },
                'best_model_path': best_model_path,
                'training_complete': True
            }
            
            # æ·»åŠ åˆ°å·²æœ‰ç»“æœæ–‡ä»¶
            results_filepath = f"{results_dir}/{results_filename}"
            if os.path.exists(results_filepath):
                try:
                    with open(results_filepath, 'r', encoding='utf-8') as f:
                        all_results = json.load(f)
                    
                    # æ·»åŠ æœ€ç»ˆæµ‹è¯•ç»“æœä½œä¸ºé¢å¤–æ¡ç›®
                    all_results.append(final_test_results)
                    
                    # ä¿å­˜æ›´æ–°åçš„ç»“æœ
                    with open(results_filepath, 'w', encoding='utf-8') as f:
                        json.dump(all_results, f, indent=4)
                    
                    print(f"æœ€ç»ˆæµ‹è¯•ç»“æœå·²æ·»åŠ åˆ° {results_filepath}")
                except Exception as e:
                    print(f"ä¿å­˜æœ€ç»ˆæµ‹è¯•ç»“æœæ—¶å‡ºé”™: {e}")
            
            print(f"æœ€ç»ˆæµ‹è¯•è¯„ä¼°: Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}, AUC: {test_auc:.4f}")
    else:
        print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶ {best_model_path}ï¼Œè·³è¿‡æœ€ç»ˆæµ‹è¯•è¯„ä¼°")
    
    print(f"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}, æ¨¡å‹ä¿å­˜åœ¨: {best_model_path}")
    return best_val_acc

# set seed
def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.cuda.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    cudnn.deterministic = True

# log
class Logger(object):
    def __init__(self, filename='default.txt', stream=sys.stdout):
        self.terminal = stream
        self.log = open(filename, 'w')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        pass

if __name__ == '__main__':
    setup_seed(2021)
    sys.stdout = Logger('result.txt', sys.stdout)
    sys.stderr = Logger('error.txt', sys.stderr)
    main()
